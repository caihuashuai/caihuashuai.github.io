<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Noise-contrastive estimation</title>
  <meta name="description" content="Interesting paper: Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/nce/">
  <link rel="alternate" type="application/rss+xml" title="J. Mark Hou" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <!-- <a class="site-title" href="/">J. Mark Hou</a> -->
    <h2 class="site-title">J. Mark Hou</h2>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
<!--           
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
           -->
            <a class="page-link" href="/">Posts</a>
            <a class="page-link" href="/about">About</a>
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Noise-contrastive estimation</h1>
    <p class="post-meta">
      <time datetime="2017-11-03T22:00:00-04:00" itemprop="datePublished">
        
        Nov 3, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Interesting paper: <a href="https://michaelgutmann.github.io/assets/papers/Gutmann2012a.pdf">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</a></p>

<p>Roughly, the paper says that:</p>
<ul>
  <li>Suppose you want to estimate some probability distribution.</li>
  <li>You can do maximum likelihood or some other standard thing.</li>
  <li>Or, you can do this thing called noise-contrastive estimation (NCE):
    <ol>
      <li>You generate some fake data according to (almost) any distribution.</li>
      <li>You estimate a logistic regression to distinguish the real data from the fake data.</li>
    </ol>
  </li>
  <li>This estimate effectively gives you the probability distribution you wanted.</li>
</ul>

<p>In this post I’ll (1) go over why this is useful, (2) provide some intuition on how it works, (3) walk through a simple example implementation in python, (4) cover some more technical details, and (5) briefly touch how this applies to estimating conditional distributions.</p>

<h3 id="1-why-this-is-useful">1. Why this is useful</h3>
<ol>
  <li>Often, you’ll estimate a probability distribution <script type="math/tex">\mathbb{P}</script> defined on some set <script type="math/tex">W</script> by trying to learn an un-normalized quantity <script type="math/tex">Q(w)</script> for each <script type="math/tex">w\in W</script>, and then normalizing it to produce a candidate distribution that actually sums to 1:
\begin{equation}
P(w) = \frac{Q(w)}{\sum_{v\in W}Q(v)}
\end{equation}
and then checking if this <script type="math/tex">P</script> looks kind of similar to the data you have (which is generated from the real distribution <script type="math/tex">\mathbb{P}</script>)</li>
  <li>So, every time you update your <script type="math/tex">Q</script>, you have to do <script type="math/tex">\sum_{v\in W}Q(v)</script> in order to compute this <script type="math/tex">P</script> so you can see how good it is (and also probaby to compute gradients so you can figure out how to make your <script type="math/tex">Q</script> better next time).</li>
  <li>You’ll probably have to try out a bunch of different <script type="math/tex">Q</script>s to get something that looks pretty good.</li>
  <li>This sum is hard if <script type="math/tex">W</script> is big, so you should try to avoid doing it.</li>
  <li>NCE is a fairly elegant way to estimate your un-normalized <script type="math/tex">Q</script> without having to do this big sum.</li>
</ol>

<h4 id="for-concreteness-consider-this-example">For concreteness, consider this example:</h4>
<ul>
  <li>Suppose you’re trying to estimate how often your friend says various words.</li>
  <li>Define this as a distribution <script type="math/tex">\mathbb{P}</script> over the set of all words <script type="math/tex">W</script>.</li>
  <li>You have some data generated from this distribution (i.e. transcripts of what your friend said over the past week).</li>
  <li>You think that maybe your friend prefers words that are:
    <ul>
      <li><script type="math/tex">\leq 5</script> letters long,</li>
      <li>or end with the letter ‘y’</li>
      <li>or have at least 3 repeated leters</li>
    </ul>
  </li>
  <li>Thus, you represent each word <script type="math/tex">w\in W</script> as a vector of features <script type="math/tex">x_w</script> and estimate the <script type="math/tex">\mathbb{P}</script> as a function of the features (so e.g. ‘lullaby’ would get turned into the vector <script type="math/tex">(0,1,1)</script>).</li>
  <li>You specify a softmax model, so that each feature in your feature vector <script type="math/tex">x_w</script> has some effect on usage probabilities, encapsulated by the vector <script type="math/tex">\theta</script> (which is the same length as <script type="math/tex">x</script>), and you assume there is some true <script type="math/tex">\theta^\ast</script> for which your model of <script type="math/tex">\mathbb{P}</script> matches the real thing
\begin{equation}
P(w|\theta) = \frac{\exp(x_w\cdot\theta)}{\sum_{v\in W}\exp(x_{v}\cdot\theta)},  \qquad P(w|\theta^\ast) = \mathbb{P}(w) 
\end{equation}</li>
  <li>You just need to estimate <script type="math/tex">\theta^\ast</script> to figure out what <script type="math/tex">\mathbb{P}</script> is, so go ahead and write down your log likelihood function and optimize this over your data via gradient descent or whatever:
\begin{equation}
\arg\min_{\theta} \left[\frac{\sum_{i=1}^N x_{w_i}\cdot\theta}{N} - \log\left(\sum_{v\in W}\exp(x_{v}\cdot\theta)\right)\right]
\end{equation}</li>
  <li>But:
    <ul>
      <li>In order to compute this log likelihood, you’ll need to compute <script type="math/tex">\sum_{v\in W}\exp(x_{v}\cdot\theta)</script>.</li>
      <li>There are a lot of words, so this sum is very time consuming.</li>
      <li>Gradient descent or whatever takes many steps, so you have to do this sum many times.</li>
    </ul>
  </li>
  <li>:(</li>
  <li>If only there were some way to estimate <script type="math/tex">\theta^\ast</script> without having to do this sum!</li>
</ul>

<h3 id="2-intuition">2. Intuition</h3>
<p>The intution is really quite straightforward, and doesn’t require any knowledge of how this method actually works beyond the brief 2-sentence description I gave in the intro.</p>
<ul>
  <li>Let <script type="math/tex">\mathbb{P}^r</script> be the real distribution that you want to estimate.</li>
  <li>Let <script type="math/tex">\mathbb{P}^f</script> be the fake distribution you use to generate the fake data.</li>
  <li>Let <script type="math/tex">W</script> be the support of both of these distributions.</li>
  <li>Assume you have a lot of real data generated by <script type="math/tex">\mathbb{P}^r</script>, and generate an equal amount of fake data from <script type="math/tex">\mathbb{P}^f</script>.</li>
  <li>Then, if you randomly pick some observation from your combined fake+real data, then the posterior probability that the observation is real, conditional on the observation being <script type="math/tex">w</script>, is going to be roughly:
\begin{equation}
p(real\mid w) = \frac{\mathbb{P}^r (w) } {\mathbb{P}^r(w)+\mathbb{P}^f(w)}
\end{equation}</li>
  <li>You train your logistic regression to distinguish real from fake data.  Since you have a lot of data, it should be able to figure out what 
<script type="math/tex">p(real\mid w)</script> is for each <script type="math/tex">w</script>.</li>
  <li>You know <script type="math/tex">\mathbb{P}^f(w)</script> since this is some distribution you made up.</li>
  <li>So you can now back out 
\begin{equation}
\mathbb{P}^r(w) = \frac{p(real\mid w)}{1-p(real\mid w)}\mathbb{P}^f(w)
\tag{1}
\label{eq:backout}
\end{equation}</li>
</ul>

<p>When I first read this paper I was surprised that you could pick <em>literally any distribution</em>(almost) and have this work.  It turns out, roughly what’s going on is that you’re using this fake distribution as a basis for comparison, learning how the real distribution compares to the fake one, and then using this to back out the real distribution.</p>

<h3 id="3-example-implementation">3. Example implementation</h3>
<p>I’ve written a basic implementation of NCE applied to the example described above.  <a href="https://github.com/j-mark-hou/nce_basic/blob/master/nce_basic.ipynb">You can look at the notebook / download and run it here</a>.</p>

<p>For less trivial examples, you can look in the paper.</p>

<p>What this example does:</p>
<ol>
  <li>Generate a set <script type="math/tex">W</script> of 10000 words
    <ul>
      <li>For each <script type="math/tex">w\in W</script>, generating a feature vector <script type="math/tex">x_w \in \{0,1\}^3</script> (indicating whether or not <script type="math/tex">w</script> has or does not have each of the 3 features listed in the example above).</li>
      <li>Generate the real probability distribution, where the probability of each word is a logistic function of the 3 features multiplied by some coefficients <script type="math/tex">\theta^\ast</script>:
\begin{equation}
 \mathbb{P}^r(w) = \frac{\exp(x_w\cdot \theta^\ast)}{\sum_{v\in W}\exp(x_v\cdot\theta^\ast)} 
 \tag{2}
 \label{eq:softmax_ex}
\end{equation}</li>
    </ul>
  </li>
  <li>Randomly generate 20000 real observations from <script type="math/tex">\mathbb{P}^r</script> that I will model as this softmax.</li>
  <li>Randomly generate 20000 fake observations from a fake distribution <script type="math/tex">\mathbb{P}^f</script> (uniform over the 10000 words).</li>
  <li>Join the real and fake data into a single dataset.</li>
  <li>Tun logistic regression on this combined data to predict if an observation is real/fake using the where I model the probability of <script type="math/tex">w</script> being real as a function of the feature vector <script type="math/tex">x</script> and a constant
<script type="math/tex">p(real\mid w) = 1/\left(1 + \exp(-(x_w\cdot\theta+c))\right)</script></li>
  <li>Compare the estimated coefficients <script type="math/tex">\hat{\theta}</script> with the true ones <script type="math/tex">\theta^\ast</script> (they’re pretty close).</li>
  <li>For each word, use the estimated logistic regression to compute <script type="math/tex">p(real\mid w)</script>, and use that to back out the estimated value of <script type="math/tex">\mathbb{P}^r(w)</script> using equation \eqref{eq:backout}.
\begin{equation}
\mathbb{P}^r(w) = \frac{p(real\mid w)}{1-p(real\mid w)}\mathbb{P}^f(w)
\end{equation}</li>
  <li>Compare the estimated values of <script type="math/tex">\mathbb{P}^r(w)</script> to the real ones (they’re pretty close).</li>
</ol>

<p>A few notes about the implementation:</p>
<ul>
  <li>Once you have <script type="math/tex">\hat{\theta}</script>, you can just as well plug this directly into equation \eqref{eq:softmax_ex} in place of <script type="math/tex">\theta^\ast</script> and compute your estimates of <script type="math/tex">\mathbb{P}^r(w)</script> that way.  this won’t give the same answer (as doing this explicitly normalizes the estimated probabilities where the method used in the notebook does not), but it’ll be close.</li>
  <li>The implementation contains a parameter <script type="math/tex">\nu</script>, which controls the ratio of how many fake observations to generate for each real observation.  If this is set to something other than 1, then the RHS of \eqref{eq:backout} needs to be multiplied by <script type="math/tex">\nu</script>.  As a result, the implementation in the notebook of \eqref{eq:backout} contains an extra <script type="math/tex">\nu</script> term.</li>
</ul>

<h3 id="4-more-details">4. More Details</h3>
<p>To be precise, NCE says you can do either of these things</p>
<ol>
  <li>MLE estimation of a distribution parameterized by <script type="math/tex">\theta</script> on real data, and</li>
  <li>Logistic regression on real+fake data parameterized by <script type="math/tex">\theta</script> plus a constant term.</li>
</ol>

<p>And the resulting estimate of <script type="math/tex">\theta</script> that we get will be the same asymptotically.  That is, if we parameterize our model of the true distribution as</p>

<script type="math/tex; mode=display">P(w\mid \theta) = \frac{Q(w\mid\theta)}{\sum_{v\in W}Q(v\mid\theta)}</script>

<p>and we let <script type="math/tex">X_r</script> and <script type="math/tex">X_f</script> denote the real and fake data respectively, then we have</p>

<script type="math/tex; mode=display">\left[\arg\max_{\theta} \left(\sum_{w\in X_r}\log P(w\mid\theta)\right)\right]  
\approx
\left[
    \arg\max_{\theta,c} \left(\sum_{w\in X_r}\log\left(\frac{1}{1+\exp(-\log Q(w\mid\theta)-c)}\right) \\
    \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
    + \sum_{w\in X_f}\log\left(\frac{1}{1+\exp(\log Q(w\mid\theta)+c)}\right)\right)
\right]</script>

<p>So that we can work with the right side problem because it allows us to just with the un-normalized <script type="math/tex">Q</script> instead of the normalized <script type="math/tex">P</script>.</p>

<p>Several notable things about NCE:</p>
<ul>
  <li>NCE works pretty much whenever maximum likelihood works.  The conditions for identification / consistency / asymptotic normality are quite weak, and look very similar to those for MLE.</li>
  <li>In order for NCE to work, the fake distribution must include the support of the real distribution.  You can see this from \eqref{eq:backout}: clearly if <script type="math/tex">\mathbb{P}^f(w)=0</script>, then you’re not going to be able to back out <script type="math/tex">\mathbb{P}^r(w)</script>.</li>
  <li>The further <script type="math/tex">\mathbb{P}^f(w)</script> is from <script type="math/tex">\mathbb{P}^r(w)</script>, the more difficult it gets to accurately estimate <script type="math/tex">\mathbb{P}^r(w)</script>.  Intuitively, if there’s a large gap between these two, then your logistic regression isn’t really incentivized to get this difference exactly right.  So, try and have a fake distribution that resembles the real one (e.g. in our example above, it might be reasonable to use the overall occurrence of words on Wikipedia as the fake distribution).</li>
  <li>The more fake observations you generate relative to real observations, the more accurate your estimate becomes.  In the limit, it becomes as accurate as MLE!  Of course, this comes at the cost of increased computational burden, which is why we didn’t like MLE in the first place.</li>
</ul>

<h3 id="5-conditional-distributions">5. Conditional distributions</h3>
<ul>
  <li>Estimating a single distribution is nice, but…</li>
  <li>Typically, what we care about is actually estimating distributions conditional on some context.  Examples:
    <ul>
      <li>Predicting the next word in a sentence conditional on the last few words.</li>
      <li>Predicting in which geographical region the next taxi ride request is going to occur conditional on recent history of rides.</li>
    </ul>
  </li>
  <li>This is a problem, because NCE introduces a normalizing parameter <script type="math/tex">c</script> in additional to the original <script type="math/tex">\theta</script> parameters of the likelihood.</li>
  <li>Typically, contexts are not often repeated in a data set.</li>
  <li>This causes the number of parameters in your model to grow about as quickly as data size, which makes NCE infeasible.</li>
  <li>In spite of this, NCE finds usage in some fairly high-profile machine learning applications, e.g. word2vec. <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">See section 2.2 of this paper</a>
    <ul>
      <li>In that implementation, the normalizing parameter <script type="math/tex">c</script> appears to be set just uniformly to 1 for all contexts (<a href="https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf">see this paper</a>), which seems like it just corresponds to fitting a normalized probability model.
        <ul>
          <li>These neural network models have a lot of parameters, so this is ok.</li>
          <li>But why do NCE? Why not just straight up learn your normalized model?</li>
          <li>I guess the probabilities are not forced to uniformly sum to 1 for all parameter values, so this is some way of coaxing the model to eventually be normalized as we update.</li>
        </ul>
      </li>
      <li>Furthermore, the actual implementation of word2vec uses a simplified version of NCE called ‘negative sampling’ that doesn’t have the nice theoretical properties of nce.
        <ul>
          <li>In the case of learning word embeddings, you’re less interested in consistency and more interested in whether your objective function can generate useful embeddings, so you’re happy to do something a bit ad-hoc so long as it works.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>At some point in the future I’ll probably look into this in more detail.</li>
</ul>


  </div>

  
</article>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">J. Mark Hou</h2> -->

    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li>
          
            <a href="mailto:jmarkhou@jmarkhou.com"><span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 .02c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.99 6.98l-6.99 5.666-6.991-5.666h13.981zm.01 10h-14v-8.505l7 5.673 7-5.672v8.504z"/></svg></span><span class="username"></span></a>

          

          
            <a href="https://github.com/j-mark-hou"><span class="icon icon--github"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg></span><span class="username"></span></a>

          

          
            <a href="https://linkedin.com/in/j-mark-hou-8ba79555"><span class="icon icon--linkedin"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></span><span class="username"></span></a>

          

          
          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
