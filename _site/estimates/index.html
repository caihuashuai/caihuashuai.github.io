<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>What are parameter estimates even</title>
  <meta name="description" content="People often try and estimate things. For example, people at tech companies tend to run a lot of experiments in order to estimate the impact of various polic...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://0.0.0.0:4000/estimates/">
  <link rel="alternate" type="application/rss+xml" title="J. Mark Hou" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <!-- <a class="site-title" href="/">J. Mark Hou</a> -->
    <h2 class="site-title">J. Mark Hou</h2>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
<!--           
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
           -->
            <a class="page-link" href="/">Posts</a>
            <a class="page-link" href="/about">About</a>
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">What are parameter estimates even</h1>
    <p class="post-meta">
      <time datetime="2017-11-29T02:00:00+00:00" itemprop="datePublished">
        
        Nov 28, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>People often try and estimate things.  For example, people at tech companies tend to run a lot of experiments in order to estimate the impact of various policies (e.g. new website layout) on important-ish things (e.g. user signup rate).  The process generally works like this:</p>
<ol>
  <li>run your experiment and get some data</li>
  <li>use your data to build an estimate of the impact of your policy</li>
  <li>assume that this estimate is your best proxy for the true impact</li>
  <li>base your decisions on this estimate of the impact (e.g. move to the new website layout if the impact on conversion is sufficiently positive)</li>
</ol>

<p>As it turns out, step 3 is actually not quite correct, and your estimate is in general <em>not</em> your best proxy for the true impact.  I recently saw a presentation by <a href="https://sites.google.com/site/tomcunningham/home">Tom Cunningham</a> and <a href="https://research.fb.com/people/coey-dominic/">Dominic Coey</a> from Facebook Research discussing their attempts at solving this problem.  The result is straightforward (and quite cute), but is somewhat unintuitive on first glance.  This unintuitiveness also leads to some interesting organizational challenges in practice.  I’ll describe these things here.</p>

<h3 id="intuition">Intuition</h3>
<ul>
  <li>Let <script type="math/tex">\gamma</script> be the true impact, and <script type="math/tex">\hat{\gamma}</script> be your estimate.</li>
  <li>If your experiment was designed correctly, then your estimate should just be the truth plus some uncorrelated noise: \begin{equation}
\mathbb{E}[\hat{\gamma}\mid\gamma]=\gamma
\end{equation}</li>
  <li>However, <em>we don’t actually care about this quantity</em>.  Instead, what we want is the best estimate of <script type="math/tex">\gamma</script> given our estimate. That is:
\begin{equation}
\mathbb{E}[\gamma\mid\hat{\gamma}]
\end{equation}</li>
  <li>In general, <script type="math/tex">\mathbb{E}[\gamma\mid\hat{\gamma}] \neq \hat{\gamma}</script>, so we can probably do better than just using <script type="math/tex">\hat{\gamma}</script> as a proxy for the true value <script type="math/tex">\gamma</script>.</li>
</ul>

<p>That’s really all there is to it.  From a Bayesian perspective this is quite clear.  Your estimate is going to be the true value plus some noise: <script type="math/tex">\hat{\gamma}=\gamma+\epsilon</script>, so conditional on the estimate <script type="math/tex">\hat{\gamma}</script>, there’s some distribution of possible true <script type="math/tex">\gamma</script>s that could have generated this <script type="math/tex">\hat{\gamma}</script>, and in general this distribution isn’t going to average out to exactly <script type="math/tex">\hat{\gamma}</script>.</p>

<p>It’s worth noting a few things:</p>
<ul>
  <li>This has nothing to do with endogeneity / your experiment being bad.</li>
  <li>This has nothing to do with selection bias / somehow only seeing large estimates / people exaggerating their effect sizes.</li>
</ul>

<h3 id="probably-shrink-your-estimates-but-not-always">Probably shrink your estimates (but not always)</h3>
<p>Alright, so your estimate <script type="math/tex">\hat{\gamma}</script> is going to be an imperfect proxy for the truth <script type="math/tex">\gamma</script>.  Most of the time, your estimate will be too extreme and will need to be shrunk down, but sometimes they’ll be too conservative and will need to be scaled up.  Some examples to illustrate this:</p>

<h4 id="1-probably-shrink-your-estimates">1. Probably shrink your estimates</h4>
<p>On average, your parameter estimates are going to be over-biased because of noise.  Consider this example: let <script type="math/tex">\hat{\gamma}=\gamma+\epsilon</script> with <script type="math/tex">\gamma</script> and <script type="math/tex">\epsilon</script> mean-<script type="math/tex">0</script> and independent.  If you fit a linear regression of <script type="math/tex">\gamma</script> on <script type="math/tex">\hat{\gamma}</script>
\begin{equation}
\gamma = a+b\hat{\gamma}
\end{equation}
then the coefficients <script type="math/tex">a,b</script> will be just 
\begin{equation}
a=\mathbb{E}[\gamma]=0, \quad b = \frac{cov(\gamma, \hat{\gamma})}{var(\hat{\gamma})} = \frac{var(\gamma)}{var(\gamma)+var(\epsilon)}&lt;1
\end{equation}
That is, if you restrict yourself to just applying some uniform scaling to your estimates, then the best adjustment (in a squared loss sense) is to shrink your estimate towards the mean.  The noisier your estimate is, the more you should shrink it.  Even though <script type="math/tex">E[\hat{\gamma}\mid\gamma]=\gamma</script>, you should still shrink it.</p>

<p>A few points:</p>
<ul>
  <li>The mean-zero assumption is WLOG since you can just… demean everything.</li>
  <li>This is basically just attenuation bias / errors in variables.</li>
</ul>

<p>We can go ahead and simulate this.  We’ll assume all noise is Gaussian because that’s easy, though the above result doesn’t require any parametric assumptions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">gamma_hat</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gamma_hat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">gamma</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gamma_hat</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">gamma_hat</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">gamma_hat</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xhat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xhat</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'gamma_hat'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'coefficient on gamma_hat: {:.3f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>
<p><img src="http://0.0.0.0:4000/assets/images/2017-11-27-linear.png" alt="linear" /></p>

<p>As expected, the coefficient on <code class="highlighter-rouge">gamma_hat</code> here is around 1/2 since the variance of the truth and the variance of the noise are equal.</p>

<h4 id="2-but-not-always">2. But not always</h4>
<p>So, on average, you should probably shrink down your estimates.  But there are clear situations where you should scale them up instead.  Consider this example:</p>
<ul>
  <li>the distribution of true parameter value is bimodal</li>
  <li>your parameter estimate ends up being just below one of the modes</li>
  <li>thus, the true value is probably a bit closer to that mode than your parameter estimate</li>
  <li>so, you should probably scale your estimates up a bit</li>
</ul>

<p>We can also go and simulate this, where we make the distribution of true parameter values a mixture of two Gaussians centered on either side of the origin.  Thus, parameter estimates near 0 should be scaled up, whereas parameter estimates far from zero should be scaled down:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">statsmodels.nonparametric.kernel_regression</span> <span class="kn">import</span> <span class="n">KernelReg</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">loc</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="n">loc</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">),</span>
                       <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)])</span>
<span class="n">gamma_hat</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>

<span class="n">kr</span> <span class="o">=</span> <span class="n">KernelReg</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">gamma_hat</span><span class="p">,</span> <span class="n">var_type</span> <span class="o">=</span><span class="s">'c'</span><span class="p">,</span> <span class="n">reg_type</span> <span class="o">=</span> <span class="s">'lc'</span><span class="p">,</span> <span class="n">bw</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">gamma_hat</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">xhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">gamma_hat</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">gamma_hat</span><span class="o">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">kr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xhat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xhat</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'gamma_hat'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'gamma'</span><span class="p">)</span>
</code></pre></div></div>
<p><img src="http://0.0.0.0:4000/assets/images/2017-11-27-upscale.png" alt="upscale" /></p>

<p>The slope of the local regression is quite high around 0, indicating that the estimates should be scaled up around 0, whereas further away from 0 the slope is lower.</p>

<h3 id="what-to-do-in-practice">What to do in practice?</h3>
<p>Ok, so it seems like it might make sense to scale your parameter estimates up/down (mostly down).  But how do you actually implement something like this in practice?  If we could observe the true <script type="math/tex">\gamma</script> in addition to our estimates <script type="math/tex">\hat{\gamma}</script>, then we could just try and estimate <script type="math/tex">\mathbb{E}[\gamma\mid \hat{\gamma}]</script> with some kind of regression like we did in the examples above.  That’s too bad, because <script type="math/tex">\gamma</script> is clearly not observable, but the Facebook guys proposed a not unreasonable method for how to do this anyway:</p>
<ol>
  <li>suppose you want to run an experiment</li>
  <li>break your population into two groups</li>
  <li>literally run the experiment twice in parallel on each of the two groups</li>
  <li>get two estimate of the true parameter from the two groups: <script type="math/tex">\hat{\gamma}_1=\gamma+\epsilon_1</script>, <script type="math/tex">\hat{\gamma}_2=\gamma+\epsilon_2</script></li>
  <li>do this every time you run any experiment</li>
  <li>take all of this data you have now, fit some kind of regression of <script type="math/tex">\hat{\gamma}_1</script> on <script type="math/tex">\hat{\gamma}_2</script></li>
  <li>use your predicted value of <script type="math/tex">\hat{\gamma}_1</script> given <script type="math/tex">\hat{\gamma}_2</script> from your fitted regression as your best proxy for the true effect size</li>
  <li>use this predicted value to inform decisions instead of the raw estimates</li>
</ol>

<p>Under the assumption that <script type="math/tex">\epsilon_1</script> and <script type="math/tex">\epsilon_2</script> are independent of each other and of <script type="math/tex">\gamma</script>, then you have
\begin{equation}
\mathbb{E}[\hat{\gamma}_1\mid \hat{\gamma}_2] = \mathbb{E}[\gamma+\epsilon_1 \mid \gamma+\epsilon_2] = \mathbb{E}[\gamma\mid \hat{\gamma}_2]
\end{equation}
Which is exactly what we’re after.  So if the errors are orthogonal then you can just do some regression to get the conditional expectation of one estimate on the other, which gives you the conditional expectation of the true parameter value given your estimate.  For the most part, this makes a lot of sense.  Some potential issues:</p>
<ul>
  <li>The error terms <script type="math/tex">\epsilon_1, \epsilon_2</script> can be correlated because of e.g. common time shocks (as in, maybe all effect sizes are just randomly bigger during the summer).  This would cause the <script type="math/tex">\epsilon</script>s to be correlated, and thus this conditional expectation to be more extreme than the truth: <script type="math/tex">\lvert\mathbb{E}[\hat{\gamma}_1\mid\hat{\gamma}_2]\rvert > \lvert\mathbb{E}[\gamma\mid\hat{\gamma}_2]\rvert</script>.</li>
  <li>You’re breaking your sample in two, so if your experiment is about market-level dynamics that has weird scaling with size, then the half-size estimates won’t really be what you’re after.</li>
</ul>

<p>Absent these issues, you can just do this two-sample split and use some estimate of <script type="math/tex">\mathbb{E}[\hat{\gamma}_1\mid\hat{\gamma}_2]</script> as your best proxy for the true effect <script type="math/tex">\gamma</script>.  So, literally do this 2-sample split procedure for every experiment, and then run some sort of local regression of <script type="math/tex">\hat{\gamma}_1</script> on <script type="math/tex">\hat{\gamma}_2</script>, and use the predicted value from your regression instead of the raw estimates.</p>

<p>But of course, it’s not that simple…</p>

<h3 id="organizational-challenges">Organizational challenges</h3>
<p>In spite of the fairly clear reasons to not use your raw parameter estimate as a proxy for the true parameter value, this is literally what almost everyone does almost all the time.  The fact that even for one of the largest and most sophisticated tech companies in the world, it’s literally the <em>research</em> group that is just now starting to figure out how to implement this should give you some indication of how widespread this practice is.  Here are some potential reasons for why people use raw parameter estimates:</p>
<ul>
  <li>This is just what people are taught.  In academia, you generally report your parameter estimates, not some Bayesian posterior mean / some otherwise adjusted thing.  So once people leave grad school, they continue doing this.</li>
  <li>It’s not entirely clear how to estimate <script type="math/tex">\mathbb{E}[\hat{\gamma}_1\mid\hat{\gamma}_2]</script>.
    <ul>
      <li>You can do a local regression, but there’s a lot of tuning parameters. How do you pick these?  Or how do you stipulate a reasonable way to pick these?</li>
    </ul>
  </li>
  <li>Why restrict to just conditioning on <script type="math/tex">\hat{\gamma}_2</script>?  Why not do 
\begin{equation}
\mathbb{E}[\hat{\gamma}_1\mid\hat{\gamma}_2, \text{ other stuff}]
\end{equation}<br />
Surely there are other useful features that can be used to get even better proxies of the truth?
    <ul>
      <li>Estimates for certain sets of parameters (e.g. marketing-related things) might tend to be more exaggerated, whereas the reverse may be true for others (e.g. website layout).  Should we include features controlling for this?</li>
      <li>Or, maybe experiments run during the winter tend to be less biased than those done in the summer?  So maybe we should also condition on season?</li>
      <li>Maybe experiments run by engineers have effects that are less exaggerated than those run by product managers.  Should we incorporate this feature?</li>
      <li>etc…</li>
      <li>And once you incorporate all of these things, how do you pick a method for computing this conditional expectation?  How do you pick tuning parameters?</li>
    </ul>
  </li>
</ul>

<p>If you’re just one person doing some analyses, then you can maybe come up with something that does pretty ok for now.  But when you’re in charge of designing a general process for how to adjust parameter estimates for a large institution, this problem becomes much harder.  Here’s a speculative list of reasons:</p>
<ul>
  <li>It’s fairly difficult to ex-ante specify the right way of resolving all of the various uncertainties in implementing some process like this.  You’ll go along and realize you’ve forgotten some edge cases and have to fix stuff as they come up.</li>
  <li>Someone whose bonus depends on ‘succeeding’ on some metric is going to be unhappy you’re shrinking their estimate down, and they’ll argue with you that things should be done in some way that benefits them.  So you’re going to have to spend a lot of time managing these conflicts.</li>
  <li>If there’s a significant number of people who aren’t happy with the policy, you’re going to have a hard time building the kind of cultural credibility to really establish some system like this.</li>
  <li>It might be pretty hard to measure/sell the impact of having improved parameter estimates to whomever is paying you, especially if you’re explicitly making life harder for many people in your organization.</li>
  <li>You might create perverse incentives where people will cheat on their experiments because if their estimates end up being bad, they’ll cause your regression to think that their entire group runs bad experiments, and thus anger their entire team / endanger their careers.</li>
</ul>

<p>These problems are probably not insurmountable, but they certainly aren’t trivial to solve on an institutional level.  So maybe it’s not so surprising that almost every place just goes with just the straight up raw parameter estimates.  They might not be great, but they’re easy, and often it’s better to be fast and ballpark than slow and exact.</p>

  </div>

  
</article>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

<!--     <h2 class="footer-heading">J. Mark Hou</h2> -->

    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          <li>
          
            <a href="mailto:jmarkhou@jmarkhou.com"><span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 .02c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.99 6.98l-6.99 5.666-6.991-5.666h13.981zm.01 10h-14v-8.505l7 5.673 7-5.672v8.504z"/></svg></span><span class="username"></span></a>

          

          
            <a href="https://github.com/j-mark-hou"><span class="icon icon--github"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg></span><span class="username"></span></a>

          

          
            <a href="https://linkedin.com/in/j-mark-hou-8ba79555"><span class="icon icon--linkedin"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></span><span class="username"></span></a>

          

          
          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>

  </body>

</html>
