---
layout: post
title:  "Graph limits and market equilibrium"
date:   2018-10-16 20:00:00 -0400
categories: statistics, economics
---

- the modern economic landscape is increasingly dominated by a few major companies built around some large private markets
	- e.g. :
		- Amazon / Alibaba for e-commerce
		- Apple / Google for apps
		- Uber / Lyft / DiDi for transportation
		- Google / Facebook / Microsoft / Amazon for ads
	- as a result, there's been some interest (from e.g. these institutions) on characterizing the equilibrium behavior of large markets
- some relevant strands of work:
	- probably the crowning achievement of classical economic theory was providing formal foundations for market equilibrium
		- this classical work seems to have least partially been motivated by a desire to provide some sort of 'scientific' justification for capitalism (this was the 50s, after all) 
		- thus, focus on abstract markets, intuitively corresponding to national-level economies
		- focused on characterizing existence / uniqueness / welfare properites of equilibria 
		- based on fixed-point arguments, so not that useful for actually finding equilibria
	- starting in the early 2000s, algorithmic game theorists have begun devising algorithms for / characterizing the complexity of computing these equilibria.
		- for some reasonable models of markets, complexity is polynomial (and sometimes even quite fast
		- but given the scale of the markets of interest described above, even writing down the entire market is a challenge
	- there's also been some recent-ish work in math / stats on characterizing the limit behavior of various properties of dense graphs
		- some conditions under which some property of a dense graph will converge as the graph gets big
		- markets are basically just bipartite graphs with the two types of vertices being consumers and products
- so, it feels like you should be able to do something like this:
	1. sample some sub-market from your big market
	2. compute equilibrium on this sub-market, and compute some smooth statistic of the equilibrium price vector
	3. the equilibrium static computed on the sampled market should converge towards the statistic on the true market as you sample larger and larger sub-markets

I'll discuss some intuition / very limited results for whether this approach makes sense




### I. Simple example
We'll be extremely concrete here and focus on a fairly trivial market with no production / only monetary endowments / specific utilities (Fisher market with Cobb-Douglas utilites).
#### I.0. Setup
- goods $$j = 1,...,J$$
	- each good has total supply $$y_j$$
	- in addition, let $$\tau_j\in\{0,1\}$$ denote binary 'type' of a good
- consumers $$i = 1,...,I$$
	- each consumer has total budget $$B_i$$ to spend on stuff
	- utility function 
	$$
	\begin{equation}\displaystyle
	u_i(x_i) = \sum_j\alpha_{ij}\log(x_{ij}), \quad\alpha_{ij}=1
	\label{cbutil}
	\tag{1}
	\end{equation}
	$$
		- a little calculus gives us that, given prices $$p = (p_1,...,p_J)$$, the optimal amount of each item to buy is 
		$$
		\begin{equation}\displaystyle
		x^*_{ij}(p) = \frac{\alpha_{ij}B_i}{p_j} 
		\label{cbopt}
		\tag{2}
		\end{equation}$$
		- thus, in particular, the total fraction of consumer $$i$$'s budget spent on good $$j$$ is just $$\alpha_{ij}$$ *independent of price*
- an equilibrium is defined by prices $$p^*$$ such that supply and demand are equal:  $$\sum_i x^*_{ij} = y_j$$
	- note that because the allocation of money to each product doesn't depend on price
	- so, the market clearing price for each good $$j$$ is just 
	$$
	\begin{equation}\displaystyle
	p^*_j = \left(\sum_i \alpha_{ij}B_i\right)/y_j
	\label{cbeq}
	\tag{3}
	\end{equation}
	$$ 
	- so this market is actually trivial
- **our aim is to figure out how much total money is spent on goods of type $$\tau_j=0$$**
	- this can be written
	$$
	\begin{equation}\displaystyle
	\sum_{j:\tau_j=0} \sum_ix^*_{ij}p^*_j = \sum_{j:\tau_j=0} \sum_i \alpha_{ij}B_i
	\label{rev0}
	\tag{4}
	\end{equation}
	$$

#### I.1. Equilibrium existence and computation
Equilibrium existence:
- this is trivial because we have closed form expression for the equilibrium price vector
- fairly general cases, classical general equilibrium results give us existences (see Arrow Debreu 1954)

Equilibrium computation:
- again, trivial because we have closed form solution
- more generally, various polynomial time algorithms for doing so:
	- tatonnement-style algorithm in exchange economies where elasticiity of substitution is not too high and consumers are endowed with money (e.g. Cole Fleischer 2008)
	- convex optimization based algorithms in the linear case (e.g. Devanur Garg Vegh 2013)

#### I.2. Sampling
- doing the sum in \eqref{rev0} requires $$O(IJ)$$ operations
- this might be large, in which case we'll probably want to sample
- if e.g. $$J$$ is small, we can just subsample consumers $$i$$ and approximate the fraction of revenue flowing to goods with $$\tau_j=0$$
	- standard LLN implies consistency
- if both $$I$$ and $$J$$ are large, then we'll need to sample in both dimensions
	- ... is this actually a thing that we can do?



### II. Graph limits and parameter testing
#### II.0. Graph limits
There's some quite elegant theory on limits of dense graphs (see Lovasz Szegedy 2004):
- a graph's adjacency matrix is just a symmetric function from $$[0,1]\times[0,1]\mapsto \{0,1\}$$ 
	- discretize the interval $$[0,1]$$ into $$\lvert V\rvert$$ intervals corresponding to the vertices of the graph
	- set the function's value to 1 on a rectangle IFF there's an edge between the two vertices corresponding to the two intervals defining the rectangle
- so instead of talking about graphs, just talk about these functions instead
- it turns out that the right limit objects for these piecewise-constant symmetric functions $$[0,1]\times[0,1]\mapsto \{0,1\}$$ are symmetric functions on $$[0,1]\times[0,1]\mapsto [0,1]$$, which are called **'graphons'**
	- the range $$[0,1]$$ can be interpreted a probability distribution on $$\{0,1\}$$

Some further work (Borgs et. al. 2007) put a norm on this graphon space (in fact, a more general space, as they allow for edge and vertex weights)
- namely, the  **cut-distance**  $$\delta_{\square}$$:
	- for two simple graphs $$G', G''$$ on the exact same (labeled) vertex set:
	$$
	\begin{equation}\displaystyle
	d_{\square}(G'', G') := \max_{S,T\subset V} \frac{1}{\lvert V\rvert^2}\left\lvert e_{G''}(S,T) - e_{G'}(S,T) \right\rvert
	\label{cutdistance0}
	\tag{5}
	\end{equation}
	$$
		- $$e_{G}(S,T)$$ is the number of edges between vertex sets $$S$$ and $$T$$
		- $$d_{\square}$$ is the maximum difference in edge density between any two vertex sub-sets
		- intuitively, this amounts to saying that $$G''$$ and $$G'$$ are 'macroscopically similar', in that no matter which edge subsets you look at, the number of edges is roughly the same
			- macroscopic because everything is scaled down by the square of the number of vertices, so that it's possible for e.g. individual vertices to have dramatically different degrees so long as the total differences are $$o(n^2)$$
		- note that this distance only makes sense for dense graphs, since otherwise this metric would be uniformly zero as the graphs got large
	- for two simple graphs $$G, G'$$ with the same number of vertices, just find the best way to relabel the vertices of $$G$$ to match the vertex set of $$G'$$, and then use the above definition:
	$$
	\begin{equation}\displaystyle
	\delta_{\square}(G, G') :=  \min_{G''\cong G} d_{\square}(G'', G')
	\label{cutdistance1}
	\tag{6}
	\end{equation}
	$$
	where $$G'' \cong G$$ refers to $$G''$$ being some re-labeling of the vertex set of $$G$$ to match the vertex set of $$G'$$
	- this definition can be generalized to graphons by doing something in the same spirit as above, but let's just stop here and use $$\delta_{\square}$$ to refer to this generalized graphon metric
- this cut-distance ends up being the right metric for graphon space, and the paper proves these results:
	- graph convergence as defined in Lovasz Szegedy 2004 is equivalent to convergence in $$\delta_{\square}$$
	- graphon space is the completion of the space of all graphs under $$\delta_{\square}$$
	- graphon space with the $$\delta_{\square}$$-metric is compact (once you collapse graphons into equivalence classes with $$\delta_{\square}=0$$)
	- with high probability, sampling a sufficiently large subgraph of some graph produces a sub-graph that is close in $$\delta_{\square}$$ to the original graph
- these results mean that questions about sampling and convergence of functions of graphs can be rephrased in terms of continuity of functionals on graphon space
	- this is cool


#### II.1. Parameter testing and continuity
- so it turns out that this idea of computing some parameters of large graph by subsampling the large graph and computing the parameter on the subsampled graph and hoping that these are close has a name: **parameter testing**
- the Borgs et. al. 2007 paper gives some very useful results for parameter testing (Theorem 6.1) in terms of continuity in $$\delta_{\square}$$
	- results are only for simple graphs, as opposed to the previous stuff which applied also to weighted graphs
	- the parameter $$f(G)$$ is some bounded real value
- the theorem states that these are equivalent (plus some other things we dont' care about):
	- (a) $$f$$ is testable
	- (d) $$f$$ can be extended to some $$\tilde{f}$$ on graphon space that is continuous in $$\delta_{\square}$$
	- (e.1-e.3):
		- (e.1) $$\forall \epsilon, \enspace \exists \epsilon'$$ such that for any two graphs $$G, G'$$ on the same vertex set with $$d_{\square}(G,G')<\epsilon'$$, we have $$\lvert f(G)-f(G')\rvert<\epsilon$$
		- (e.2) $$f(G[m])$$ converges as $$m\rightarrow\infty$$, where $$G[m]$$ represents the graph obtained by duplicating each vertex of $$G$$ $$m$$ times and adding in all the corresponding edges
		- (e.3) $$\lvert f(G')-f(G)\rvert\rightarrow 0$$ as the number of vertices in $$G$$ goes to $$\infty$$, where $$G'$$ is just $$G$$ except with an additional disconnected vertex
- so, testability is equivalent to continuity in $$\delta_{\square}$$, which is equivalent to some easy-ish-to-check conditions


### III. Testability of market equilibria
#### III.1. Simple example
Let's now apply this nice theory of parameter testing to the simple we started off with.  This basically amounts to showing that the type-0 revenue in \eqref{rev0} is a a testable graph parameter.

So let's first represent the simple example above as a graph:
- consumers $$i$$ and goods $$j$$ are vertices
- a consumer has vertex-weight corresponding to its budget $$B_i$$
- a good has vertex-weight corresponding to its supply $$y_j$$
- the edge weight between consumer $$i$$ and good $$j$$ is consumer $$i$$'s valuation of good $$j$$, which is $$\alpha_{ij}$$

As the above theory applies only to simple graphs, we'll make some more constraints:
- in order to get rid of weights for vertices coresponding to goods:
	- let all goods have the same supply, and normalize by total supply $$y_j=1/J$$
- in order to get rid of edge weights:
	- make preference binary so that a consumer $$i$$ either likes or doesn't like a good $$j$$
	- for the goods $$j$$ that consumer $$i$$ prefers, uniformly set $$\alpha_{ij}=1/\mathrm{deg}_i$$ where $$\mathrm{deg}_i$$ is number of goods $$i$$ likes
- in order to get rid of weights on vertices corresponding to consumers:
	- set the budget of consumer $$i$$ to $$\mathrm{deg}_i$$, so that people who like more stuff have more money to spend
	- it might be more natural to give consumers equal budget, but we'll discuss later why this is problematic

This then gives us a simple graph $$G$$ with:
- $$I+J$$ total vertices
- consumer-vertex $$i$$ is attached to good-vertex $$j$$ IFF consumer $$i$$ likes good $$j$$
- there are no connections between consumer-vertices, and similarly for good-vertices

So, now \eqref{rev0} can be written 
$$\sum_{j:\tau_j=0} \sum_i \frac{1}{\mathrm{deg}_i}\mathrm{deg}_i = \sum_{j:\tau_j=0} \mathrm{deg}_j$$, which is just the total number of all edges attached to goods of type $$\tau_j=0$$.  We'll also normalize this by the square of the total number of vertices to construct a graph parameter:
$$
\begin{equation}\displaystyle
R_0(G) = \frac{1}{(I+J)^2}\sum_{j:\tau_j=0} \mathrm{deg}_j
\tag{7}
\label{revparam}
\end{equation}
$$

So we need to show that $$R_0(G)$$ is testable, which we can do via conditions (e.1-e.3):
- (e.1):
	- $$R_0(G)$$ as we've constructed it is literally just the edge density between the sets of good-vertices $$j$$ with $$\tau_j=0$$ and the entire vertex set
	- the condition that \eqref{cutdistance0} be small just says that the edge density between any two subsets of vertices is small
	- which guarantees that the difference in $$R_0$$ between the two graphs will also be small
- (e.2):
	- so we replicate each vertex $$M$$ times 
	- there are now $$M$$ copies of each good-vertex $$j$$ and similarly for each consumer-vertex $$i$$
	- let $$(j,m)$$ represent a good in this new $$G[M]$$ graph, and similarly $$(i,m)$$ for consumers
	- so we have 
	$$
	R_0(G[M]) = \frac{1}{((I+J)M)^2}\sum_{(j,m):\tau_j=0} \mathrm{deg}_{(j,m)}
	= \frac{1}{M^2(I+J)^2}M\sum_{j:\tau_j=0} M\mathrm{deg}_{j} = R_0(G)
	$$
	- so duplicating the vertices $$M$$ times actually keeps $$R_0$$ the same
- (e.3):
	- adding an isolated vertex to $$G$$ will only impact $$R_0(G)$$ by changing $$\frac{1}{(I+J)^2}$$ to $$\frac{1}{(I+J+1)^2}$$
	- this change is clearly negligible as $$(I+J)\rightarrow\infty$$

Thus, it follows that $$R_0(G)$$ is testable, and thus it makes sense to approximate this in a large graph by just taking some $$G'$$ subsampled from $$G$$ and then using $$R_0(G')$$ to approximate $$R_0(G)$$



### REST



#### I.3. subsampling
- simple graph, 0/1 edges
- budget of a consumer vertex is its degree
	- so, 1 dollar for each of the goods its attached to
- 

discus why these choices
- can't do constant budget for everyone
	- suppose there's one guy with a single attachment, type0
	- he gets a second attachment, type1
	- this is an O(1/n) change in the revenue share of type1, but an O(1/n2) change in the cut norm
	- so this is infeasible
- why normalize by n2 instead of fraction of total budget:
	- if first all consumers have degree1, each attached to a single type0 vertices, and then everyone gets another attachment to type1 vertices,
	- this is small in the rectangle norm (there's an n^2 in denom)
	- but half of the mass is now on type1 whereas previously it was type0
	- so we have to rule this out => send both cases to 0


### extensions
- what if it was general normed space instead of just [0,1] for the function?
- compact-decorated


### other stuff





#### I.2. Does this seem reasonable?
- products are totally separable, in that the fraction of money spent on that product doesn't depend on any prices, so equilibrium computation is trivial
- there is a fixed set of goods and no production, and consumer are allocated money but no goods
	- seems reasonably represenative of some market over a short period of time
- this is a static market (in that there's no explicit concept of time)
	- again, reasonable for considering markets over short periods of time
- consumers have preferences over goods but not money, and thus spend all of their money on goods
	- this is less reasonable of an assumption, as you would expect consumers to... not spend all of their money?
	- not hard to relax this assumption
- quantities are infinitely divisible
	- this is an assumption made for analytical tractability, and doesn't match so well with reality
	- need to justify this on a case-by-case basis

Classical results give us:
- existence of equilibrium via Kakutani fixed point theorem
	- in this particular case, equilibrium is unique
	- so Brouwer would also work here
- upper hemicontinuity of the equilibrium price in the model parameters (e.g. $$\alpha, \sigma, B$$ and even $$I$$)
	- this is a consequence of Berge theorem for upper hemicontinuity of argmax
	- in this particular case it's just continuity, as the argmax consumption bundle $$x^*_i(p)$$ is unique

This is nice, but:
- these results don't tell us how to actually compute the equillibrium prices $$p^*$$
- these results don't tell us what hapens when both $$I,J$$ go to $$\infty$$



### II. Computing equilibrium
There's been increasing work on computing equilibria of games.  t



### III. Graph limits



### IV. Graph limits and market equilibrium
consider a case where 
- 



### References
- Arrow Debreu 1954: [ Existence of an Equilibrium for a Competitive Economy](https://web.stanford.edu/class/msande311/arrow-debreu.pdf)
- Hildenbrand Mertens 1972 [Upper Hemi-Continuity of the Equilibrium-Set Correspondence for Pure Exchange Economies](https://www.jstor.org/stable/1909724?seq=1#metadata_info_tab_contents)
- Cole Fleischer 2008: [Fast-Converging Tatonnement Algorithms for One-Time and Ongoing Market Problems](https://cs.nyu.edu/cole/papers/CF2008.pdf)
- Devanur Garg Vegh 2013 [A Rational Convex Program for Linear Arrow-Debreu Markets](https://arxiv.org/abs/1307.8037)
- [Limits of compact decorated graphs](https://arxiv.org/abs/1010.5155)
- Lovasz Szegedy 2004 [Limits of dense graph sequences](https://arxiv.org/abs/math/0408173)
- [Graph limits and parameter testing](https://www.microsoft.com/en-us/research/uploads/prod/2016/11/Graph-Limits-and-Parameter-Testing.pdf)
- Borgs et. al. 2007 [Convergent Sequences of Dense Graphs I: Subgraph Frequencies, Metric Properties and Testing](https://arxiv.org/abs/math/0702004)
