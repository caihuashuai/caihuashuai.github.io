---
layout: post
title:  "Infinitesimal jackknife for bagged estimators"
date:   2018-05-30 00:00:00 -0400
categories: statistics
---


I was initially a bit confused by the form of the infinitesimal jackknife / nonparametric delta method variance estimator used in the [Athey/Wager random forests paper](https://arxiv.org/abs/1510.04342).  Basically, when you have an estimator that's generated by averaging some base estimator over a bunch of bootstrapped samples, the (asymptotic) sampling variance of the averaged estimator can be computed like this:

1. for each observation in your data set
    - compute your base estimator on each bootstrap sample
    - count the number of times each this observation shows up in each bootstrap sample
    - compute the covariance between these two things _over the set of all bootstrap samples_
2. square each of these covariance, sum them up, and then multiply by some stuff
3. this is the sampling variance of the averaged estimator

This formula seems directionally reasonable: the covariance over the bootstrap samples of (base estimator) and (count of a given datapoint) is a measure of how much this given datapoint affects the estimate.  If these covariances are generally large, then you would expect your estimator to have relative high dependence on the data, and thus higher sampling variance.

To build a slightly deeper heuristic understanding of this, we'll:
- introduce directional derivatives for functionals of probability distributions, and how they basically just feel like regular directional derivatives
- discuss how the nonparametric delta method / infinitesimal jackknife is basically just the regular delta method, but with fancier derivatives
- apply this to bagged estimators (= estimators built by averaging a bunch of base estimators over bootstrap samples) to get the sum-of-squared-covariances formula (see \eqref{bagvar} below for the exact statement)


### I. Functional derivatives
We'll be working with statistics of distributions, which are functionals that take a probability distributions as input and output a real number.  For example, the expected value of a distribution $$\int d\mathbb{P}(x) x$$ is a natural example.  In order to motivate the nonparametric delta method, we'll need some notion of directional derivative for these functionals (i.e. the [Gateaux derivative](https://en.wikipedia.org/wiki/G%C3%A2teaux_derivative)).  This definition is basically the same as the definition of directional derivative for functions on $$\mathbb{R}^n$$, so it should be pretty intuitive.

- consider some $$T$$, some functional that maps probability distributions to real numbers
- the domain of $$T$$ is some space of probability distributions
- a direction is just an element of this space = a probability distribution
- so, let $$G$$ and $$H$$ be distributions, and let $$(1-t)G+tH$$ denote the distribution generated by taking $$G$$, removing a bit of probability from it, and then replacing that bit with $$H$$
- The 'directional derivative' of $$T$$ at $$G$$ in the direction $$H$$ is defined as:
  $$\displaystyle\begin{equation}
  T^\prime_{G}(H) := \lim_{t\rightarrow 0} \frac{T\left((1-t)G+tH\right)-T(G)}{t}
  \tag{I.1}
\label{derivdef}
  \end{equation}$$
- $$T$$ is defined as 'differentiable' at $$G$$ if:
$$\begin{equation}
\quad T^\prime_{G}(H) \text{ is linear and continuous in } H
  \tag{I.2}
\label{differentiabledef}
\end{equation}$$

Two useful properties:
- for any for any $$G$$, the direction derivative at $$G$$ in the direction $$G$$ is $$0$$:
$$
\begin{equation}
T^\prime_{G}(G) = \lim_{t\rightarrow 0} \frac{T\left((1-t)G+tG\right)-T(G)}{t} = 0
\tag{I.3}
\label{selfderivzero}
\end{equation}
$$
- by linearity, 
$$
\begin{equation}
T^\prime_{G}(H) = T^\prime_{G}\left(\int dH(x)\delta(x)\right) =\int dH(x) T^\prime_{G}\delta(x) 
\tag{I.4}
\label{derivdecomp}
\end{equation}
$$ 
where $$\delta(x)$$ be a probability distribution with all mass concentrated on the point $$x$$, so that you can decompose the directional derivative in a the direction of a probability distribution $$H$$ into a sum of a bunch of directional derivatives, each of which only depends on a single point $$x$$
    - this makes the directional derivative an average of IID objects, which is easy to analyze



### II. Parametric delta method
- Suppose we're estimating some parameter $$\theta$$, and our estimate is consistent and asymptotically normal (by central limit theorem or whatever): $$\hat{\theta} \rightarrow \theta$$, and $$\sqrt{n}(\hat{\theta}-\theta) \Rightarrow Z$$ where $$Z\sim N(0, \Sigma)$$ 
- So, we can approximate the true parameter via $$\theta \approx \hat{\theta}$$ and the distribution of our estimator via $$\sqrt{n}(\hat{\theta} - \theta) \approx \hat{Z}$$ where $$\hat{Z}=N(0, \hat{\Sigma})$$  (we're also estimating $$\Sigma$$).
  - e.g. $$\theta$$ is the population mean, then LLN and CLT give you $$\hat{\theta}$$ and $$\hat{Z}$$ 
- Often, what we care about is actually some transformation $$\phi = T(\theta)$$, and it's not as easy to get the corresponding estimates $$\hat{\phi}$$ and $$\sqrt{n}(\hat{\phi}-\phi)$$ (e.g. because you can't directly apply CLT and LLN because $$\phi$$ is not an average)
- So instead, we just plug in the estimator of $$\theta$$ into $$T$$ to get $$\hat{\phi} := T(\hat{\theta})$$, and use the delta method to get $$\sqrt{n}(\hat{\phi}-\phi) = T^\prime(\theta)\cdot Z$$, after which we plug in estimates of $$\phi$$ and $$Z$$ to get $$\sqrt{n}(\hat{\phi}-\phi) \approx T^\prime(\hat{\theta})\cdot \hat{Z}$$

The parametric delta method says if $$\sqrt{n}(\hat{\theta} - \theta) \rightarrow Z$$ and $$\phi=T(\theta)$$, then $$\sqrt{n}(\hat{\phi}-\phi) \rightarrow T^\prime(\theta)\cdot Z$$.

This is basically just a Taylor expansion: 
- $$T(\hat{\theta}) \approx T(\theta) + T^\prime(\theta)(\hat{\theta}-\theta)$$,
- so, $$\sqrt{n}(T(\hat{\theta}) - T(\theta)) \approx T^\prime(\theta)\sqrt{n}(\hat{\theta}-\theta) \Rightarrow T^\prime(\theta)\cdot Z$$

Or, in words:
- for $$\hat{\theta}$$ near  $$\theta$$, $$T$$ is approximately linear: $$T(\hat{\theta}) \approx T(\theta) + T^\prime(\theta)(\hat{\theta}-\theta)$$
- thus, for a given $$\hat{\theta}$$, $$\sqrt{n}(T(\hat{\theta}) - T(\theta)) \approx T^\prime(\theta)\sqrt{n}(\hat{\theta}-\theta)$$
- so, let's sample a random direction $$z$$ from $$Z=\sqrt{n}(\hat{\theta}-\theta)$$, then compute the directional derivative of $$T$$ in this direction, which is $$T^\prime(\theta) \cdot z$$, which is approximately the corresponding value of $$\sqrt{n}(\hat{\phi}-\phi)$$
- this then allows us to go from a distribution for $$\sqrt{n}(\hat{\theta}-\theta)$$ to a distribution for $$\sqrt{n}(\hat{\phi}-\phi)$$


### III. Nonparametric delta method / infinitesimal jackknife
The above applies when we can express $$\phi$$ as a function of some nice finite-dimensional parameter.  More generally, we might care about estimating some $$\phi$$ that's a some function of an entire distribution:  $$\phi = T(\mathbb{P})$$.  In this case, we should be able to essentially do the same thing as the parametric case:
1. approximate $$\phi = T(\mathbb{P})$$ with $$\hat{\phi} = T(\hat{\mathbb{P}})$$
3. approximate $$\sqrt{n}(\hat{\phi} - \phi) = \sqrt{n}(T(\hat{\mathbb{P}}) - T(\mathbb{P}))$$ with some directional derivative via Taylor expansion
2. characterize the asymptotic distribution of this directional derivative
4. approximate this asymptotic distribution to get $$\sqrt{n}(\hat{\phi}-\phi)$$


To be a bit more detailed about these steps:
1. approximate $$\phi = T(\mathbb{P})$$ with $$\hat{\phi} = T(\hat{\mathbb{P}})$$:
    - this should be fine so long as $$T$$ is continuous (for some reasonable definition of continuity on this space)
2. approximating $$\sqrt{n}(\hat{\phi}-\phi)$$ with directional derivative:
    - since we have some notion of a directional derivative now, it seems reasonable to do a Taylor expansion of $$T(\hat{\mathbb{P}})$$ around $$\mathbb{P}$$, which gives us
        $$\begin{equation} 
        T(\hat{\mathbb{P}}) \approx T(\mathbb{P})) + T^\prime_{\mathbb{P}}(\hat{\mathbb{P}}) \Rightarrow \sqrt{n}(T(\hat{\mathbb{P}}) - T(\mathbb{P})) \approx  \sqrt{n}T^\prime_{\mathbb{P}}(\hat{\mathbb{P}}) 
        \end{equation}$$
    as desired
3. characterizing the asymptotic distribution of $$\sqrt{n}T^\prime_{\mathbb{P}}(\hat{\mathbb{P}})$$:
    - $$T^\prime_{\mathbb{P}}(\hat{\mathbb{P}}) = \int d\hat{\mathbb{P}}(x) T^\prime_{\mathbb{P}}(\delta_x) = \frac{1}{n}\sum_iT^\prime_{\mathbb{P}}(\delta_{x_i})$$, by \eqref{derivdecomp}
    - this thing is an average of IID RVs, so we're going to just do the CLT
    - in order to do the CLT, we need the expectation and variance of 
     $$T^\prime_{\mathbb{P}}(\delta_{X})$$ for $$X\sim \mathbb{P}$$
    - the expectation is 0: by linearity of $$T^\prime$$ and $$\int d \mathbb{P}(x) \delta_x = \mathbb{P}$$, and \eqref{selfderivzero}, we have 
        $$ \mathbb{E}[T^\prime_{\mathbb{P}}(\delta_{X})] = \int d\mathbb{P}(x)T^\prime_{\mathbb{P}}(\delta_{x}) = T^\prime_{\mathbb{P}}({\mathbb{P}}) = 0$$ 
    - the variance: we'll just denote it by $$V:= \mathbb{V}(T^\prime_\mathbb{P}(\delta_{X}))) = \int d\mathbb{P}(x)(T^\prime_\mathbb{P}(\delta_{x}))^2 $$
        - we'll compute this on a case-by-case basis for each $$T$$
    - so, the CLT now gives us
    $$\sqrt{n}\frac{1}{n}\sum_i\left(T^\prime_{\mathbb{P}}(\delta_{x_i}) - 0\right) \Rightarrow \mathcal{N}(0,V)$$ as desired
4. approximate this asymptotic distribution:
    - $$V$$ in the asymptotic distribution above require knowing the true $$\mathbb{P}$$, so instead we'll just estimate it by plugging in $$\hat{\mathbb{P}}$$:\\
    $$\displaystyle\hat{V} = \int d\mathbb{\hat{P}}(x)(T^\prime_\hat{\mathbb{P}}(\delta_{x}))^2 = \frac{1}{n}\sum_{i=1}^n(T^\prime_\hat{\mathbb{P}}(\delta_{x}))^2$$

Thus, we have the nonparametric delta method: for $$\phi = T(\mathbb{P})$$ with $$T$$ sufficiently nice:
$$\begin{equation}
\phi \approx \hat{\phi} = T(\hat{\mathbb{P}}), \quad
\sqrt{n}(\hat{\phi} - \phi) \approx \mathcal{N}(0,\hat{V}),
\text{ where } \hat{V} = \frac{1}{n}\sum_{i=1}^n(T^\prime_\hat{\mathbb{P}}(\delta_{x}))^2
\tag{III.1}
\label{nonparametricdeltamethod}
\end{equation}$$

$$T^\prime_\hat{\mathbb{P}}(\delta_{x})$$ is also called the 'influence function' of point $$x$$ on $$T$$, since it measures how much $$T$$ changes when you perturb $$\mathbb{P}$$ by adding a small point mass at $$x$$.  Thus, the nonparametric delta method variance formula just says that the asymptotic variance of $$T$$ the averaged squared influence functions.  Again, this is sensible, since if $$T$$ changes a lot with small changes $$\mathbb{P}$$, you would expect it to be asymptotic variance to be larger.  


### IV. Relation to the jackknife

The nonparametric delta method is also called the 'infinitesimal jackknife', since it actually looks like an ordinary jackknife, except infinitesimal.

The ordinary jackknife estimates $$\phi=T(\mathbb{P})$$ and $$\sqrt{n}(\hat{\phi} - \phi)$$ by:
- generating $$n$$ estimates of $$\phi$$, each of which involves doing some bias correction using a leave-one-out sample of the data
- averaging these $$n$$ estimates to approximate $$\phi$$
- using the distribution of these $$n$$ estimates to approximate $$\sqrt{n}(\hat{\phi} - \phi)$$

More precisely:
- the leave-one-out distribution resulting from just removing a given observation $$x_i$$ can be written as
$$\hat{\mathbb{P}}_{(i)} := (1-t) \hat{\mathbb{P}} + t \delta_{x_i}$$ with $$t=-1/(n-1)$$
- we can generate an estimate of $$\phi$$ by applying $$T$$ to this leave-one-out distribution, and then using the resulting thing to de-bias the main one:
$$\begin{equation}
\displaystyle\hat{\phi}_{(i)} := T(\hat{\mathbb{P}}) - \frac{T(\hat{\mathbb{P}}) - T(\hat{\mathbb{P}}_{(i)})}{t}
\tag{IV.1}
\label{pseudovalue}
\end{equation}$$
    - note that if you substitute in for $$t$$, you get $$nT(\hat{\mathbb{P}}) - (n-1)T(\hat{\mathbb{P}}_{(i)})$$, which is $$n$$ times an estimate minus $$n-1$$ times another one, so the scale seems right.
- then, you average these debiased leave-one-out estimates to get your final estimator for $$\phi$$, :\\
$$\begin{equation}
\displaystyle \hat{\phi}_{(.)} := \frac{1}{n}\sum_i\hat{\phi}_{(i)}
\tag{IV.2}
\label{OJKEstimate}
\end{equation}$$
- finally, assume that these $$\hat{\phi}_{(i)}$$ are independent and compute/approximate the asymptotic distribution as you normally would:\\
$$\begin{equation}
\displaystyle\sqrt{n}\left(\hat{\phi}_{(.)} - \phi\right) \approx \mathcal{N}(0, \hat{V}), 
\quad  \hat{V} = \frac{1}{n}\sum_i\left(\hat{\phi}_{(i)}-\hat{\phi}_{(.)}\right)^2 \approx \mathbb{V} \hat{\phi}_{(i)}
\tag{IV.3}
\label{OJKVariance}
\end{equation}$$
    - note that this independence assumption isn't that insane: if $$T$$ is linear, then $$\hat{\phi}_{(i)}$$ is just $$T(\delta_{x_i})$$, in which case this independence is legit.  For nonlinear $$T$$, you can think of doing a linear approximation, seems like it should be fine if  $$t=1/(n-1)$$ is small and $$T$$ is smooth.

To do the infintesimal jackknife, the intuition is the same, except that instead of omitting each observation, we downweight it by some small amount.  This amounts to just taking $$t\rightarrow 0$$ instead of the fixed $$t=-1/(n-1)$$ we had in the ordinary jackknife:
- equation \eqref{pseudovalue} from above would instead become\\
 $$\displaystyle\hat{\phi}_{(i)} := T(\hat{\mathbb{P}}) - \lim_{t\rightarrow 0}\frac{T(\hat{\mathbb{P}}) - T\left((1-t) \hat{\mathbb{P}}+t\delta_{x_i}\right)}{t} = T(\hat{\mathbb{P}}) - T^\prime_{\hat{\mathbb{P}}}(\delta_{x_i})$$
- as a result, \eqref{OJKEstimate} becomes a bit simpler:
$$\displaystyle \hat{\phi}_{(.)} := \frac{1}{n}\sum_i\hat{\phi}_{(i)} = T(\hat{\mathbb{P}}) - \frac{1}{n}\sum_iT^\prime_{\hat{\mathbb{P}}}(\delta_{x_i}) = T(\hat{\mathbb{P}}) - T^\prime_{\hat{\mathbb{P}}}(\hat{\mathbb{P}}) = T(\hat{\mathbb{P}}) $$ since  $$T^\prime_{\hat{\mathbb{P}}}(\hat{\mathbb{P}})=0$$
- and the variance estimator in \eqref{OJKVariance} becomes:\\
$$\displaystyle \hat{V} = \frac{1}{n}\sum_i\left(\hat{\phi}_{(i)}-\hat{\phi}_{(.)}\right)^2 =
\frac{1}{n}\sum_i (T^\prime_{\hat{\mathbb{P}}}(\delta_{x_i}))^2$$

Note that these infinitesimal jackknife estimates/variances correspond exactly to the nonparametric delta method ones in \eqref{nonparametricdeltamethod}.  Thus, the infinitesimal jackknife is exactly the nonparametric delta method, and the ordinary jackknife is an approximation of it.


### V. Nonparametric delta method for bagged estimators
Boostrap aggregation (bagging) involves averaging an estimator over a bunch of bootstrapped samples of some data.  Some notation:
- Let $$S$$ be the bagged estimator, generated from averaging base estimators $$T$$ on a bunch of bootstrapped samples
- We have data $$x_1,..,x_n$$ 
- This data defines an empirical distribution $$\mathbb{P}_n$$
    - This is a categorical distribution supported on $$x_1,..,x_n$$ with probability $$1/n$$ for each
    - Similarly, we can define any a general categorical distribution $$\rho$$ on $$x_1,..,x_n$$ where the probability of $$x_i$$ is $$\rho_i$$
- Let $$\mathbb{B}^{\rho}$$ be the distribution over possible bootstrap samples generated by taking $$n$$ IID draws from a categorical distribution $$\rho$$ over $$x_1,..,x_n$$ 
    - Let $$\tilde{x}:= (\tilde{x}_1,...,\tilde{x}_n)$$ be a sample generate from $$\mathbb{B}^{\rho}$$ 
    - Let $$\tilde{y}_i:=\#\{j:\tilde{x}_j=x_i\}$$ denote number of times $$x_i$$ appears in the bootstrapped sample $$(\tilde{x}_1,...,\tilde{x}_n)$$

Then, the bagged estimator $$S$$ is defined

$$
\begin{equation}
S(\mathbb{P}_n):=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}T(\tilde{x})
\label{bagdef}
\tag{V.1}
\end{equation}
$$

Two points:
- $$S$$ seems like it should be smooth, since you're averaging it over all bootstrapped samples, so it shouldn't move too much if the distribution changes a little.  
- $$S$$ is an average of $$T$$s, so that $$S$$ should converge to $$T(\mathbb{P})$$ as you get more and more data.

So it seems reasonable to apply the nonparametric delta-method expression in \eqref{nonparametricdeltamethod} here to get the sampling variance of $$S(\mathbb{P}_n)$$:  

$$\hat{V} = \frac{1}{n}\sum_i\left(S^\prime_{\mathbb{P}_n}(\delta_{x_i})\right)^2$$

So basically we just need to compute the influence functions $$S^\prime_{\mathbb{P}_n}(\delta_{x_i})$$
- Let $$\rho:=(1-t)\mathbb{P}_n+t\delta_{x_i}$$
    - this is a categorical distribution on $$(x_1,...,x_n)$$ with probability $$\rho_i = \frac{1+(n-1)t}{n}$$ of $$x_i$$ and  $$\rho_j = \frac{1-t}{n}$$ for all $$x_j\neq i$$
- So $$S(\rho) = \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\rho}}T(\tilde{x})$$
- So we can write \\
$$
\begin{align}
S^\prime_{\mathbb{P}_n}(\delta_{x_i}) 
&= \lim_{t\rightarrow 0} \frac{S(\rho)-S(\mathbb{P}_n)}{t} \newline
&= \lim_{t\rightarrow 0} \frac{1}{t}\left(\mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\rho}}T(\tilde{x})-\mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}T(\tilde{x})\right) \newline
&= \lim_{t\rightarrow 0} \frac{1}{t}\sum_{\tilde{x}}\left(\mathbb{B}^{\rho}(\tilde{x}) - \mathbb{B}^{\mathbb{P}_n}(\tilde{x}) \right) T(\tilde{x})\newline
&= \lim_{t\rightarrow0}\frac{1}{t} \sum_{\tilde{x}}\mathbb{B}^{\mathbb{P}_n}(\tilde{x})\left(\frac{\mathbb{B}^{\rho}(\tilde{x})}{\mathbb{B}^{\mathbb{P}_n}(\tilde{x})}-1 \right)T(\tilde{x})\newline
&=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[\lim_{t\rightarrow0}\frac{1}{t}\left(\frac{\mathbb{B}^{\rho}(\tilde{x})}{\mathbb{B}^{\mathbb{P}_n}(\tilde{x})} -1 \right)T(\tilde{x})\right]\newline
&=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[\lim_{t\rightarrow0}\frac{1}{t}\left(\frac{\left(\frac{1+(n-1)t}{n}\right)^{\tilde{y}_i}\left(\frac{1-t}{n}\right)^{n-\tilde{y}_i}}{1/n^n} -1\right)T(\tilde{x})\right]\newline
&=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[\left(\lim_{t\rightarrow0}\frac{(1+(n-1)t)^{\tilde{y}_i}(1-t)^{n-\tilde{y}_i} -1}{t} \right)T(\tilde{x})\right]\newline
&=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[\left(\lim_{t\rightarrow0}
\frac{1+(n-1)t\tilde{y}_i-t(n-\tilde{y}_i) +o(t) -1}{t} \right)T(\tilde{x})\right]\newline
&=  \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[\left(\lim_{t\rightarrow0}
\frac{tn(\tilde{y}_i-1) + o(t)}{t} \right)T(\tilde{x})\right]\newline
&=  n \mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}\left[(\tilde{y}_i-1)T(\tilde{x})\right]\newline
&= n\mathrm{Cov}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}(\tilde{y}_i, T(\tilde{x})) \newline
\end{align}
$$\\
where this last equality holds because $$\mathbb{E}_{\tilde{x}\sim \mathbb{B}^{\mathbb{P}_n}}[\tilde{y}_i]=1$$, since on average $$x_i$$ will appear once in any given bootstrap sample.
- This formula for $$S^\prime_{\mathbb{P}_n}(\delta_{x_i})$$ seems plausible, since $$S$$ is just an average of $$T$$s over the set of all bootstrap samples, so the impact of $$x_i$$ on $$S$$ should presumably just be the average effect of $$x_i$$ on $$T$$ over the bootstrap samples, which is just the correlation.

So, it follows that asymptotic sampling variance of $$S(\mathbb{P}_n)$$ can be estimated by
$$
\begin{equation}
\hat{V} = \frac{1}{n}\sum_i\left(S^\prime_{\mathbb{P}_n}(\delta_{x_i})\right)^2
= n \sum_i \left[\mathrm{Cov}_{\tilde{x}\sim \text{bootstrap}}\left(\#\{j | \tilde{x}_j=x_i\}, T(\tilde{x}) \right)\right]^2
\label{bagvar}
\tag{V.2}
\end{equation}
$$




### References
- All the top hits on google for 'nonparametric delta method', all of which are seemingly based on chapter 20 of [Van der Vaart's Asymptotic Statistics](https://books.google.com/books/about/Asymptotic_Statistics.html?id=udhfQgAACAAJ&hl=en).
- Jaeckel's [original paper on the infinitesimal jackknife](http://faculty.washington.edu/fscholz/Reports/InfinitesimalJackknife.pdf).
- Efron's [2014 paper on infinitesimal jackknife for bagged estimators](http://statweb.stanford.edu/~ckirby/brad/papers/2013ModelSelection.pdf)
- Wager/Athey's [2017 paper on pointwise asymptotic normality for random forests](https://arxiv.org/abs/1510.04342)