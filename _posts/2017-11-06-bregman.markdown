---
layout: post
title:  "Poisson regression and Bregman divergence"
date:   2017-11-13 22:00:00 -0400
categories: machine learning
---

Poisson regression is a fairly useful tool for modeling count data.  From the name, you would expect that the data should be Poisson for it to work.  As it turns out, you can actually consistently estimate your parameters even when your data is not Poisson distributed.  I'll talk about how to arrive at this consistency result by means of a construct called the Bregman divergence.


### 1. What is Poisson regression and why does it exist
Poisson regression essentially amounts to two assumption: (1) conditional on some $$x$$ variables, your dependent variable $$Y$$ is distributed Poisson, and (2) the mean of this Poisson is an exponential of some function of $$x$$ (let's assume linear for now):
\begin{equation}
Y\mid x\sim Psn(\lambda_x), \quad \lambda_x = \mathbb{E}[Y\mid x] = \exp(x\cdot\theta^\ast)
\tag{1}
\label{psn}
\end{equation}
These assumptions gives you a nice concave log likelihood that you can then easily optimize given some data:
\begin{equation}
L(\theta) \propto \sum_{i\in\text{data}}\left( -\exp(x_i\cdot\theta) + y_ix_i\cdot\theta\right), \quad 
\hat{\theta} = \arg\max_{\theta} l(\theta)
\tag{2}
\label{pll}
\end{equation}

Poisson regression is a convenient thing to do when:
1. Your $$Y$$ variable takes on nonnegative integer values.
2. Your $$x$$ variables seems like they should have effect sizes that should be quoted in percentages intead of raw units (e.g. $$x$$ should increase $$Y$$ by 20%, not 20 units).

To be extremely concrete, consider this example:
- $$Y$$ is number of taxi ride requests in an area, $$x_1$$ is population, and $$x_2\in\{0,1\}$$ indicates rain
- If you do a linear regression $$\mathbb{E}[Y] = \theta_1 x_1 + \theta_2x_2$$, the model assumes that the impact of rain $$\theta_2$$ is just some constant irrespective of the population size.
- If you do a Poisson regression, then  $$\mathbb{E}[Y] = \exp(\theta_1 x_1 + \theta_2x_2) = \exp(\theta_1x_1)\exp(\theta_2x_2)$$, so that $$\theta_2$$ has an effect on rides that scales with population size.

Of course, there are other ways to get this multiplicative form:
- Linear regression on $$\log(Y)$$.  But:
  - You can't log zeros.
  - This will get you an estimate of $$\mathbb{E}[\log(Y)\mid x]$$, and in general $$\mathbb{E}[Y\mid x]\neq\exp(\mathbb{E}[\log(Y)\mid x])$$.
- Manually specify interaction terms in your linear regressions, e.g. estimate $$\mathbb{E}[Y] = \theta_1 x_1 + \theta_2x_2 + \theta_3x_1x_2$$.  But:
  - Kind of annoying having to do this a bunch of times.
  - Hard to estimate a specification more flexible than just "scales linearly with a single variable".  In particular, if there's some linear combination of several variables that you think $$x_2$$ should be proportional to, then your model becomes $$\mathbb{E}[Y] = \text{stuff}+(\theta_1 x_1 + \theta_3x_3)\theta_2x_2$$, which is kind of weird and nonlinear and less trivial to estimate than some kind of regression.

Point is, this exponential function form is often extremely convenient when you're modeling count data.


### 2. You don't actually need the Poisson
We indicated above that Poisson regression is estimated by maximizing the log likelihood.  This is nice, but a little bit worrying, since if you want to actually estimate $$\mathbb{E}[Y\mid x]$$ correctly via maximum likelihood, generally you need that your likelihood function is actually correct. Unfortunately, the assumption that $$Y\mid x$$ is Poisson is unlikely to be even close to true in practice:
- The Poisson distribution is extremely restrictive in that it has literally a single parameter that you can vary (the mean), and the variance must be equal to the mean.  
- Even if the truth is that your $$Y$$ is Poisson conditional on some $$x$$ variables, if one of these $$x$$ variables is hidden so that you don't have data on it, then the resulting distribution of $$Y$$ conditioned on the $$x$$ variables you do have data on is generally not going to be Poisson.  

Thankfully, for Poisson regression, you actually can consistently estimate $$\mathbb{E}[Y\mid x]$$ even when $$Y\mid x$$ is not Poisson, so long as the functional form for this mean is correct. As in, if in fact $$\mathbb{E}[Y\mid x] = \exp(x\cdot\theta^\ast)$$ for some $$\theta^\ast$$, then you're totally fine.  Slap whatever error distribution you want on it, doesn't matter.  Thus, as far as consistency is concerend, you can just think about the mean, which is generally much easier than thinking about the entire distribution.

There's a variety of ways to demonstrate this fact, but a super convenient way is by means of this thing called the Bregman divergence, which is in its own right a quite interesting construct.



### 3. Bregman divergence
Bregman divergence is a concept introduced in the 60s that's found some interesting applications in machine learning (like proving consistency of various clustering algorithms, as [1] does).  To generate a Bregman divergence, you first pick some strictly convex function, and then you construct another function that basically measures how convex this first function is.  To be precise, the bregman divergence $$D_{\phi}$$ associated with a strictly convex function $$\phi$$ is defined as:
\begin{equation}
D_{\phi}(x,y) = \phi(x) - \phi(y) - \nabla\phi(y)\cdot(x-y)
\tag{3}
\label{bregman} 
\end{equation}
This has a clear interpretation:  $$\phi(y) + \nabla\phi(y)\cdot(x-y)$$ is the value of the linear approximation to $$\phi$$, using the point $$y$$, evaluated at the point $$x$$.  Thus, $$D_{\phi}(x,y)$$ measures the gap at the point $$x$$ between the actual function $$\phi$$ and its linear approximation using the point $$y$$.  

The most straightforward example of a Bregman divergence is the $$L_2$$ distance.  You can derive this by just plugging into the definition:
\begin{equation}
\phi(x)= x\cdot x \quad \Rightarrow \quad D_{\phi}(x,y) = (x-y)\cdot(x-y)
\tag{4}
\label{l2breg}
\end{equation}


Also, here are some properties of Bregman divergences:
- $$D_{\phi}(x,y)$$ is always nonnegative, and 0 only when $$x=y$$ (because $$\phi$$ is strictly convex).
- $$D_{\phi}(x,y)$$ is strictly convex in the first argument (because $$\phi$$ is strictly convex), but not necessarily in the second.
- $$D_{\phi}(x,y)$$ is generally not symmetric.
- For any random variable $$X$$, the expected bregman divergence is minimized at the mean:
\begin{equation}
\arg\min_{y}\mathbb{E}[D_{\phi}(X,y)] = \mathbb{E}[X]
\tag{5}
\label{bregmin}
\end{equation}

This last property is what we'll use to prove Poisson consistency under misspecification, so we'll pay a bit more attention to it.  It's notable that this holds for *any* Bregman divergence $$D_{\phi}$$.  It doesn't matter what you use, so long as it's a Bregman divergence, the mean is the minimizer.  The proof of this fact is actually trivial, so I'll just put it here:
- Let $$P$$ denote the distribution function of $$X$$, and $$\mu:=\mathbb{E}[X] = \int dP(x)x$$ the mean. 
- It suffices to show that $$\Delta := \mathbb{E}[D_{\phi}(X,y)] - \mathbb{E}[D_{\phi}(X,\mu)] \geq 0$$ for any $$y$$:
\begin{align}
\Delta &=  \int dP(x)[(\phi(x)-\phi(y)-\nabla\phi(y)\cdot(x-y)) - (\phi(x)-\phi(\mu)-\nabla\phi(\mu)\cdot(x-\mu))]\newline
&= \int dP(x)[\phi(\mu)-\phi(y)-\nabla\phi(y)\cdot(x-y) + \nabla\phi(\mu)\cdot(x-\mu)]\newline
&= \int dP(x)[\phi(\mu)-\phi(y)]-\int dP(x)[\nabla\phi(y)\cdot(x-y)] + \int dP(x)[\nabla\phi(\mu)\cdot(x-\mu)]\newline
&= \phi(\mu)-\phi(y)-\nabla\phi(y)\cdot(\mu-y) + \nabla\phi(\mu)\cdot(\mu-\mu)\newline
&= D_{\phi}(\mu, y)\newline
&\geq 0
\end{align}

So, one immediate implication of this property is that the squared error is minimized by the mean:
$$(x-y)\cdot(x-y)$$ is a Bregman divergence, so irrespective of how $$X$$ is distributed, we have that

$$\arg\min_{y}\mathbb{E}[(X-y)\cdot(X-y)] = \mathbb{E}[X]$$

Something similar applies to the Poisson case, as we'll show below.


### 4. Poisson consistency without the Poisson
To show that Poisson regression is consistent even when the data is not Poisson, we'll show that the Poisson log likelihood can be written as a Bregman divergence, from which it follows that the mean will minimize it (and then do a little bit extra stuff).

Consider $$Y\mid x$$ for any arbitrary $$x$$, and assume it has true distribution $$P(y)$$ and true mean $$ \lambda_x = \mathbb{E}^P[y] = \int dP(y)y$$.  
- Under the assumption that $$Y\mid x \sim Psn(\lambda_x)$$, the negative log-likelihood can be written as 
\begin{align}
-l(\lambda, y) &= -y\log(\lambda) +\lambda - \log(y!) \newline
 & = -y\log(\lambda) + \lambda + y\log(y) -y + g(y)
\end{align}
for some $$g(y)$$ that we don't care about because the purpose of the log likelihood is to be minimized as a function of $$\lambda$$.
- Now, observe that if we define $$\phi(x) = x\log(x)$$, then $$D_{\phi}(y,\lambda) = y\log(y)-y\log(\lambda) -y + \lambda $$
- So it follows that $$-l(\lambda, y) = D_{\phi}(y, \lambda) + g(y)$$
- Thus, maximizing the expected Poisson log likelihood is equivalent to minimizing the expected Bregman divergence:
\begin{equation}
\arg\max_{\lambda} \mathbb{E}^P[l(\lambda, Y)] =  \arg\min_{\lambda} \mathbb{E}^P[D_{\phi}(Y, \lambda)] = \lambda_x
\end{equation}
- That is, irrespective of what the true distribution of $$Y\mid x$$ actually is, if you assume that $$Y\mid x$$ is Poisson and write down the log likelihood, the minimizer of this will the expected value.

Now, because our model was correctly specified and in fact there is some $$\theta^\ast$$ for which $$\lambda_x=x\cdot \theta^\ast$$ for all $$x$$, it follows that this $$\theta^\ast$$ will maximize the log likelihoods of $$Y\mid x$$ for each $$x$$, and therefore maximize the overall Poisson log likelihood.  From here, it follows that as you get more and more data, your empirical distribution of $$(x,y)$$ will look more and more like the true one, so in the limit you'll recover the true $$\theta^\ast$$, and thus consistency is established.  (Note: to actually write down a proof, there's some amount of technical stuff you'll have to do involving uniform convergence and continuity of argmins and uniqueness of $$\theta^\ast$$ and what else have you.  But honestly it's been a while since grad school, and the intuition is roughly just this, so whatever.)

And we're done.  You can now go do Poisson regression in peace.

A few more notable points:
- This result is just for consistency.  If you want confidence intervals then the ones you get out of Poisson regression are going to be incorrect if your distribution is not actually Poisson.
- The linear form we assumed for $$\log(\mathbb{E}[Y\mid x])$$ actually doesn't play any role in this result.  Though, if you use something nonlinear, it's worth bearing in mind how this would impact the concavity of your log likelihood.
- It's important to have the functional form of $$\mathbb{E}[Y\mid x]$$ correctly specified in order for the consistency result to hold.  While this is a less demanding condition relative to getting the entire distribution right, it's still kinda hard.
- This result says nothing about rate of convergence.  If the true distribution is actually Poisson, then you can't do better than Poisson regression (since in that case it's MLE), but otherwise you can gain something by knowing more information about the distribution.  So maybe think about that if your data is extremely non-Poisson.
- It turns out that there's quite a lot of distributions (the exponential family) whose log likelihoods can be written as Bregman divergences, and as a result MLE is actually mean consistent under an assumption that the distribution is something from this family.  For example,  the Gaussian is member of this family, and the $$L_2$$ distance is the Bregman divergence corresponding to the Gaussian distribution (which makes sense, since linear regression is just MLE under the assumption of Gaussian errors).  The paper in the reference has a more general treatment of this correspondence between exponential families and Bregman divergence.




### References
[1]. ["Clustering with Bregman Divergences", JMLR 2005](http://www.jmlr.org/papers/volume6/banerjee05b/banerjee05b.pdf)