---
layout: post
title:  "Nonparametric / deep instrumental variables"
date:   2018-03-06 22:00:00 -0400
categories: statistics
---


Here's a pretty fun [paper on deep instrumental variables](http://proceedings.mlr.press/v70/hartford17a/hartford17a.pdf).  Some reasons why it's fun:
1. deep learning is pretty hype
2. instrumental variables (and causal inference more generally) is becoming pretty hype
3. paper is pretty easy to understand
3. deep learning is actually pretty ancillary to the central thrust of the paper

I'll discuss what instrumental variables is, and how this deep IV paper approaches the problem.


### Instrumental variables (IV)
People are often interested in how things affect other things: 
1. If you sell stuff, you might care about how prices affect number of sales.  
1. If you run a social media company, you might be interested in how the number of ads you show affect how much people use your platform.
1. If you work in education policy, you might care about how education affects peoples' lifetime earnings.

This general situation can be described as follows:
- $$y = g(x) + \epsilon$$, where $$y$$ and $$x$$ are observable but $$\epsilon$$ is not
- you're trying to learn what $$g$$ is from your data.  

To see how each of the examples above maps to the formula above:
1. $$y$$ is how many units something sold, $$x$$ is the price.
2. $$y$$ is the number of hours someone spends on your social media site, $$x$$ is the fraction of their news feed that's ads.
3. $$y$$ is somebody's total lifetime earnings, $$x$$ is their education level.

So, the most natural thing you can do to figure out how $$x$$ affects $$y$$ is to just... look at your data.  As in, for a given $$x$$, you can look at all the various $$y$$s associated with that, and then just average them to get
$$\hat{g}(x):=\mathbb{E}_n[y|x]$$.  It's clear that if $$\mathbb{E}[\epsilon \mid x]=0$$, then $$\hat{g}(x)\rightarrow g(x)$$ for all $$x$$, so that as you get more and more data you'll eventually learn the function $$g$$.  

This method of requires that $$\mathbb{E}[\epsilon \mid x]=0$$.  Unfortunately, this assumption is often violated.  For each of the above examples:
1. The higher priced items are also higher quality, and higher quality items sell more, so that quality drives a part of $$\epsilon$$, so that $$\epsilon$$ is correlated with price $$x$$.
2. Your social media platform's machine learning team has already optimized ad targeting a bit, so that people that get shown more ads are naturally more patient, and more patient people spend more time on your platform, so that patience drives a part of $$\epsilon$$, so that $$\epsilon$$ is correlated with number of ads the person sees $$x$$.
3. People who are more intelligent tend to get more education, but intelligence itself also affects how much you earn, so that intelligence drives a part of $$\epsilon$$, so that $$\epsilon$$ is correlated with education level $$x$$.

To deal with this problem, people often turn to something called 'instrumental variables'.  An instrumental variable $$z$$ is something such that:
- $$z$$ affects $$x$$
- $$z$$ doesn't affect $$y$$ via any other channels, only through its impact on $$x$$
- $$z$$ is independent of $$\epsilon$$

In the pricing example, you might run an experiment where you randomly discount some products by some percentage.  The discount amount has an obvious affect on the price $$x$$, is independent of quality or anything else affecting $$\epsilon$$ (because it's random), and (probably) doesn't affect purchases except through price.  So the discount percentage would be your instrument $$z$$.

With your instrument, you can basically do something like this:
- look at variation in $$z$$
- this variation in $$z$$ affects $$x$$, which then affects $$y$$
- so, the change in $$y$$ when $$z$$ varies must be due purely to changes $$x$$, since by assumption:
	- $$z$$ has no effect on $$\epsilon$$, and 
	- $$z$$ has no other impact on $$y$$
- this allows you to deduce the impact of $$x$$ on $$y$$.

We'll be more formal about this in the next section.


### Nonparametric IV, more formally
(Note: you can see [Newey Powell 2003](https://eml.berkeley.edu//~powell/npiv.pdf) for more details.  This is called 'nonparametric' because we're treating the function $$g$$ as a general function, as opposed to being defined by a fixed set of parameters.)  

Alright, let's formally define our setup:
\begin{equation}
y=g(x)+\epsilon,\quad \mathbb{E}[e\mid z]=0, \quad \text{possibly } \mathbb{E}[e\mid x]\neq 0
\tag{1}
\label{ivdef}
\end{equation}
- $$y, x, z$$ are all observable, $$\epsilon$$ is not
- $$y$$ is 1-dimensional, $$x$$ and $$z$$ can be multidimensional
- $$y$$ is your outcome variable, $$x$$ are covariates, $$z$$ are instruments

So, the formal version of the argument given at the end of the previous section is basically integrating the expression in \eqref{ivdef}:
\begin{equation}
\mathbb{E}[y\mid z] = \int dF(x\mid z) g(x)
\tag{2}
\label{fredholm}
\end{equation}
Some points:
- $$\epsilon$$ drops out because $$\mathbb{E}[e\mid z]=0$$.
- $$F(x\mid z)$$ is observable, since it's just the distribution of $$x$$ given $$z$$.
- $$\mathbb{E}[y\mid z]$$ is observable, since it's just the average of $$y$$ given $$z$$.
- So, the problem of figuring out what $$g$$ is amounts to solving for $$g$$ given this equation.

Alright, let's do some examples of equation \eqref{fredholm} to fix intuition:
- Suppose $$x=z$$.  
	- In that case, $$F(x\mid z)$$ is just the delta distribution, supported purely on $$x=z$$.  
	- So, \eqref{fredholm} reduces to $$\mathbb{E}[y\mid z] =  g(x)\mid_{x=z}$$, which is just $$\mathbb{E}[y\mid x] =  g(x)$$.  
	- This makes sense, since if $$x=z$$ then there's no need for an instrument.  Your $$x$$ is independent of $$\epsilon$$.  So you can just literally look at $$\mathbb{E}[y\mid x]$$ to get $$g(x)$$. 
- Suppose $$z$$ has no impact on $$x$$, as in $$F(x\mid z) = F(x)$$ for all $$z$$.  
	- In that case, \eqref{fredholm} becomes $$\mathbb{E}[y\mid z] = \int dF(x) g(x)$$ which implies that $$\mathbb{E}[y\mid z] = \mathbb{E}[g(x)]$$.  
	- Thus, you're not going to be able to learn $$g(x)$$ in this case (unless $$g$$ is constant).
	- This is also intuitive: if your instrument $$z$$ doesn't have any impact on your $$x$$, then it's not possible for you to deduce the impact of $$x$$ on $$y$$ via this instrument because changing this instrument doesn't change your $$x$$ at all. 
- Suppose $$g(x)=x\cdot\beta$$, and also that $$\mathbb{E}[x\mid z]=Az$$ for some vector $$\beta$$ and $$A$$ where $$A$$ has rank $$length(x)$$.
	- \eqref{fredholm} becomes $$\mathbb{E}[y\mid z] = \int dF(x\mid z) x\cdot\beta$$ which implies $$\mathbb{E}[y\mid z] = \mathbb{E}[x\mid z]\cdot\beta = (Az)\cdot\beta$$.
	- You can estimate $$A$$ since $$\mathbb{E}[x\mid z]=Az$$ and you observe $$\mathbb{E}[x\mid z]$$ and $$z$$.
	- You can estimate $$\beta$$ since you observe $$Az$$ and $$\mathbb{E}[y\mid z]$$ and $$Az$$ spans the same space as $$x$$ as you vary $$z$$.
	- This is just your typical linear instrumental variables setup, where in your first stage you estimate how your $$z$$ affects your $$x$$ (which is the $$A$$), and then in your second stage where you plug in your first-stage estimate of $$x$$ given $$z$$ (which is $$Az$$) to estimate $$\beta$$.

### Identification? Estimation?
Given that we want to back out $$g$$ using \eqref{fredholm}, it would be nice if there were some identification conditions telling us whether/when there's only a single function $$g$$ that satisfies \eqref{fredholm} out of some class of functions $$\mathcal{G}$$.  Identification holds IFF a 'completeness' condition holds.  Roughly this completeness condition amounts to a requirement that any random variable $$\tilde{g}(x)$$ for $$\tilde{g}\in \mathcal{G}$$ must somehow not exploit any information that's present in $$x$$ but not in $$z$$:
- Note that any function $$\tilde{g}$$ that satisfies $$\eqref{fredholm}$$ must have $$\mathbb{E}[y\mid z] = \int dF(x\mid z) \tilde{g}(x)$$
- So $$0 = \mathbb{E}[\tilde{g}(x)-g(x) \mid z]$$.
- By definition $$g$$ is identified exactly when $$\mathbb{E}[\tilde{g}(x) \mid z]=\mathbb{E}[g(x) \mid z]$$ implies $$\tilde{g}=g$$.  This last thing is the 'completeness' condition.

Let's look again at the various examples above to understand this completeness condition:
- If $$x=z$$, then clearly fixing $$z$$ exactly pins down $$g(x)=g(z)$$, so this condtion holds, and as thus $$g$$ is identified.
- If $$z$$ has no impact on $$x$$, then any $$\tilde{g}$$ that averages out to $$\mathbb{E}[g]$$ is going to satisfy $$\mathbb{E}[\tilde{g}(x) \mid z]=\mathbb{E}[g(x) \mid z]=\mathbb{E}[g(x)]$$, so that we will not be able to identify any $$g$$ that's not constant.
- If $$g(x)=x\cdot\beta$$, and also that $$\mathbb{E}[x\mid z]=Az$$ with $$A$$ full rank, then $$x$$ may contain more information than $$z$$, but the full rank condition guarantees that varying $$z$$ induces variation in $$x$$ in as many directions as $$x$$ itself, so that the $$\mathbb{E}[x\mid z]$$ spans the same space as $$x$$.  Then, because $$g=x\cdot\beta$$ is linear, the extra information that $$x$$ has that's not in $$z$$ is irrelevant, as variation in $$\mathbb{E}[x\mid z]$$ suffices to recover $$\beta$$, so that $$g$$ is identified.


There are some theoretical results on this completeness condition, though maybe not as extensive as we'd like.
- There are fairly simple situations where completeness fails (see [Severini Tripathi 2006](http://web2.uconn.edu/tripathi/published-papers/et-ident.pdf)). 
- But completeness is 'generic' in some sense so long as you have as many instruments $$z$$ as regressors $$x$$ (see [Andrews 2011](https://core.ac.uk/download/pdf/6339904.pdf)).  
	- The sense in which completeness is generic is an extension of the standard 'measure-1 wrt Lebesgue' definition extended to infinite dimensional spaces.  
	- The genericness is relative to a fairly expansive looking class of distributions (as in, consider the set of all distributions in this fairly expansive class, then the complete ones are generic).  
	- This seems like a pretty useful result that suggests we shouldn't worry too much, though I'm not entirely convinced I trust my intuition about these kind of unnatural infinite-dimensional objects.
- Also, it's impossible to perform hypothesis testing for whether completeness holds, based on observations of $$x,z$$ (see [Canay Santos Shaikh 2013](http://www.econ.ucla.edu/andres/pdfs/testingid.pdf)).
- Though this untestability is due to the presence of incomplete distributions that are arbitrarily close to any complete distribution, and if you kind of lump these arbitrarily-close-to-being-complete distributions with the complete ones, testing becomes possible (see [Freyberger 2017](https://www.ssc.wisc.edu/~jfreyberger/Completeness_Freyberger.pdf)).


In practice, identification is good, but if we want to estimate $$g$$ we need some sort of consistency result.  On this front, we have the problem that the solution to \eqref{fredholm} may not be a continuous function of $$\mathbb{E}[y\mid z]$$ and $$F(x\mid z)$$.  This discontinuity is a problem, because typically we'll estimate stuff by getting a bunch of data so that our empirical versions of $$\mathbb{E}[y\mid z]$$ and $$F(x\mid z)$$ converge towards the truth, and then we invoke an argument by continuity that our resulting estimate of $$g$$ will also converge towards the truth as we get more data.  If we have no guarantee of this continuity then there's no guarantee that that our estimate of $$g$$ will converge to the truth even if our estimates of $$\mathbb{E}[y\mid z]$$ and $$F(x\mid z)$$ become arbitrarily good. You can often circumvent this kind of issue by regularizing things appropriately, but in that case you'll have to argue consistency on a case by case basis.  For example, Newey Powell 2003 provides some consistency results assuming that $$g$$ is some linear combination of some basis functions + $$L^2$$ regularization. 

Overall, general theory for identification/estimation of nonparametric IV is somewhat incomplete, so that we don't have the kind of totally solid guarantees that maybe we'd like.


### Deep IV
Thankfully, we're now at a point in history where it you can often just throw function approximators at various problems and have it work pretty well even if it's not entirely clear why.  So, even with a lack of theoretical guarantees about identification / estimation, it's not a bad idea to just go ahead and just try to do something reasonable, and hope that things work out.

The Deep IV paper proposes to do a very reasonable thing:
1. Do standard supervised learning to estimate $$F(x\mid z)$$.  This is the 'first stage'.
1. Use your $$\hat{F}(x\mid z)$$ as a proxy for the truth $$F(x\mid z)$$, find $$\hat{g}$$ so that $$\mathbb{E}[y\mid z] = \int d\hat{F}(x\mid z) \hat{g}(x)$$ is approximately satisfied in your data.  This is the 'second stage'

This is quite intuitive: if we have lots of data, we can get a pretty good estimate of $$F(x\mid z)$$, and also of $$\mathbb{E}[y\mid z]$$.  We're going to hope that we live in a world where $$g$$ is identified and not too ill-posed.  If that's the case, then as we get a lot of data, we'll be able to learn $$g$$.  

This strategy is also quite nice, because it breaks the problem of estimating $$g$$ into two supervised leanring problems, both of which are fairly easy to throw function approximators at.  
1. For the first stage, both $$x$$ and $$z$$ are observable.  The complication here is that, instead of trying to predict $$\mathbb{E}[x\mid z]$$ as a function of $$z$$, you're trying to predict the entire distribution of $$x$$ given $$z$$.  Distributions are generally much more difficult to predict, particularly if $$x$$ is multidimensional, but there's various things you can do.  Two immediate examples: 
- Assume there's some parametric form for $$F(x\mid z)$$, so you're only predicting a finite number of parameters. E.g. assume normal / mixture of normals (this is what the Deep IV paper does).
- If the distribution of $$x\mid z$$ is just 1-dimensional, then you can just estimate quantiles of $$F(x\mid z)$$.
- Discretize your $$x$$-space and estimate a categorical distribution on the grid.  
2. For the second stage, this is also a supervised learning problem: You have $$N$$ observations $$y_i, x_i, z_i$$, and you're trying to find some $$\hat{g}$$ such that \eqref{fredholm} holds approximately in your data.  That is, you want to pickg $$g$$ that makes $$\mathbb{E}[y\mid z]$$ and $$\int d\hat{F}(x\mid z) \hat{g}(x)$$ very close to each other in your data.  So, you can think about minimizing the squared loss:
\begin{align}
\hat{g}&= \arg\min_{\tilde{g}}\sum_i\left(y_i - \int d\hat{F}(x\mid z_i) \tilde{g}(x))\right)^2 \newline
& \approx \arg\min_{\tilde{g}}\sum_i\left(y_i - \sum_{x_{ij}\sim \hat{F}(x_{ij}\mid z_i)} \tilde{g}(x))\right)^2
\label{obj}
\tag{3}
\end{align}
where in the second step we've replaced the internal integral with a sum, because the integral $$\int d\hat{F}(x\mid z_i) \tilde{g}(x))$$ will need to be implemented by sampling a bunch of $$x_{ij}\sim \hat{F}(x\mid z_i)$$ and then taking a sum.
- You can parameterize your $$\tilde{g}$$ as a function of some stuff, in which case you can just go ahead and do gradient descent or something like that.  The Deep IV paper uses a neural network here.
- There's some amount of complication here, in that there's a sum on the inside of your square that you need to evaluate every time you want to evaluate the objective function, and that's kind of hard to deal with.  So, one thing you can do is to just move the sum outside of the square.  This amounts to using a different objective function (that upper bounds the previous one via Jensen):
\begin{align}
\hat{g}&= \arg\min_{\tilde{g}}\sum_i\int d\hat{F}(x\mid z_i) \left(y_i - \tilde{g}(x))\right)^2 \newline
 & \approx \arg\min_{\tilde{g}}\sum_i \sum_{x_{ij}\sim \hat{F}(x_{ij}\mid z_i)} \left(y_i - \tilde{g}(x_{ij}))\right)^2
\tag{4}
\label{ezobj}
\end{align}
With this, you can just randomly draw a bunch of $$\hat{x}_{ij}$$s for each $$z_i$$ according to $$\hat{F}(x\mid z_i)$$, treat this new set of $$(y_i, \hat{x}_{ij}, z_i)$$ as your data and then just do straight-up supervised learning on this instead.  However, just swapping out one objective function for another is a bit questionable, and certainly if we had consistency before we would lose it by doing this.  But at the end of the day, everything that we're doing here is a bit suspicious from a theoretical perspective, and this swapping out kind of feels not that unreasonable.  So, worth trying out, especially because it's easy.
