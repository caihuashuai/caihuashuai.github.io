---
layout: post
title:  "Noise-contrastive estimation"
date:   2017-11-03 22:00:00 -0400
categories: machine learning
---



Interesting paper: [Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics](https://michaelgutmann.github.io/assets/papers/Gutmann2012a.pdf)

Roughly, the paper says that:
- Suppose you want to estimate some probability distribution.
- You can do maximum likelihood or some other standard thing.
- Or, you can do this thing called noise-contrastive estimation (NCE):
  1. You generate some fake data according to (almost) any distribution.
  2. You estimate a logistic regression to distinguish the real data from the fake data.
- This estimate effectively gives you the probability distribution you wanted.

In this post I'll (1) go over why this is useful, (2) provide some intuition on how it works, (3) walk through a simple example implementation in python, (4) cover some more technical details, and (5) briefly touch how this applies to estimating conditional distributions.

### 1. Why this is useful
1. Often, you'll estimate a probability distribution $$\mathbb{P}$$ defined on some set $$W$$ by trying to learn an un-normalized quantity $$Q(w)$$ for each $$w\in W$$, and then normalizing it to produce a candidate distribution that actually sums to 1:
\begin{equation}
P(w) = \frac{Q(w)}{\sum_{v\in W}Q(v)}
\end{equation}
and then checking if this $$P$$ looks kind of similar to the data you have (which is generated from the real distribution $$\mathbb{P}$$)
3. So, every time you update your $$Q$$, you have to do $$\sum_{v\in W}Q(v)$$ in order to compute this $$P$$ so you can see how good it is (and also probaby to compute gradients so you can figure out how to make your $$Q$$ better next time).
2. You'll probably have to try out a bunch of different $$Q$$s to get something that looks pretty good.
4. This sum is hard if $$W$$ is big, so you should try to avoid doing it.
5. NCE is a fairly elegant way to estimate your un-normalized $$Q$$ without having to do this big sum.

#### For concreteness, consider this example:
- Suppose you're trying to estimate how often your friend says various words.
- Define this as a distribution $$\mathbb{P}$$ over the set of all words $$W$$.
- You have some data generated from this distribution (i.e. transcripts of what your friend said over the past week).
- You think that maybe your friend prefers words that are:
  - $$\leq 5$$ letters long, 
  - or end with the letter 'y'
  - or have at least 3 repeated leters
- Thus, you represent each word $$w\in W$$ as a vector of features $$x_w$$ and estimate the $$\mathbb{P}$$ as a function of the features (so e.g. 'lullaby' would get turned into the vector $$(0,1,1)$$).
- You specify a softmax model, so that each feature in your feature vector $$x_w$$ has some effect on usage probabilities, encapsulated by the vector $$\theta$$ (which is the same length as $$x$$), and you assume there is some true $$\theta^\ast$$ for which your model of $$\mathbb{P}$$ matches the real thing
\begin{equation}
P(w|\theta) = \frac{\exp(x_w\cdot\theta)}{\sum_{v\in W}\exp(x_{v}\cdot\theta)},  \qquad P(w|\theta^\ast) = \mathbb{P}(w) 
\end{equation}
- You just need to estimate $$\theta^\ast$$ to figure out what $$\mathbb{P}$$ is, so go ahead and write down your log likelihood function and optimize this over your data via gradient descent or whatever:
\begin{equation}
\arg\min_{\theta} \left[\frac{\sum_{i=1}^N x_{w_i}\cdot\theta}{N} - \log\left(\sum_{v\in W}\exp(x_{v}\cdot\theta)\right)\right]
\end{equation}
- But:
  - In order to compute this log likelihood, you'll need to compute $$\sum_{v\in W}\exp(x_{v}\cdot\theta)$$.
  - There are a lot of words, so this sum is very time consuming.
  - Gradient descent or whatever takes many steps, so you have to do this sum many times.
- :(
- If only there were some way to estimate $$\theta^\ast$$ without having to do this sum!



### 2. Intuition
The intution is really quite straightforward, and doesn't require any knowledge of how this method actually works beyond the brief 2-sentence description I gave in the intro.
- Let $$\mathbb{P}^r$$ be the real distribution that you want to estimate.
- Let $$\mathbb{P}^f$$ be the fake distribution you use to generate the fake data.
- Let $$W$$ be the support of both of these distributions.
- Assume you have a lot of real data generated by $$\mathbb{P}^r$$, and generate an equal amount of fake data from $$\mathbb{P}^f$$.
- Then, if you randomly pick some observation from your combined fake+real data, then the posterior probability that the observation is real, conditional on the observation being $$w$$, is going to be roughly:
\begin{equation}
p(real\mid w) = \frac{\mathbb{P}^r (w) } {\mathbb{P}^r(w)+\mathbb{P}^f(w)}
\end{equation}
- You train your logistic regression to distinguish real from fake data.  Since you have a lot of data, it should be able to figure out what 
$$p(real\mid w)$$ is for each $$w$$.
- You know $$\mathbb{P}^f(w)$$ since this is some distribution you made up.
- So you can now back out 
\begin{equation}
\mathbb{P}^r(w) = \frac{p(real\mid w)}{1-p(real\mid w)}\mathbb{P}^f(w)
\tag{1}
\label{eq:backout}
\end{equation}

When I first read this paper I was surprised that you could pick *literally any distribution*(almost) and have this work.  It turns out, roughly what's going on is that you're using this fake distribution as a basis for comparison, learning how the real distribution compares to the fake one, and then using this to back out the real distribution.


### 3. Example implementation
I've written a basic implementation of NCE applied to the example described above.  [You can look at the notebook / download and run it here](https://github.com/j-mark-hou/nce_basic/blob/master/nce_basic.ipynb).

For less trivial examples, you can look in the paper.

What this example does:
1. Generate a set $$W$$ of 10000 words
  - For each $$w\in W$$, generating a feature vector $$x_w \in \{0,1\}^3$$ (indicating whether or not $$w$$ has or does not have each of the 3 features listed in the example above).
  - Generate the real probability distribution, where the probability of each word is a logistic function of the 3 features multiplied by some coefficients $$\theta^\ast$$:
\begin{equation}
    \mathbb{P}^r(w) = \frac{\exp(x_w\cdot \theta^\ast)}{\sum_{v\in W}\exp(x_v\cdot\theta^\ast)} 
    \tag{2}
    \label{eq:softmax_ex}
\end{equation}
1. Randomly generate 20000 real observations from $$\mathbb{P}^r$$ that I will model as this softmax.
1. Randomly generate 20000 fake observations from a fake distribution $$\mathbb{P}^f$$ (uniform over the 10000 words).
1. Join the real and fake data into a single dataset.
1. Tun logistic regression on this combined data to predict if an observation is real/fake using the where I model the probability of $$w$$ being real as a function of the feature vector $$x$$ and a constant
$$p(real\mid w) = 1/\left(1 + \exp(-(x_w\cdot\theta+c))\right)$$
1. Compare the estimated coefficients $$\hat{\theta}$$ with the true ones $$\theta^\ast$$ (they're pretty close).
1. For each word, use the estimated logistic regression to compute $$p(real\mid w)$$, and use that to back out the estimated value of $$\mathbb{P}^r(w)$$ using equation \eqref{eq:backout}.
\begin{equation}
\mathbb{P}^r(w) = \frac{p(real\mid w)}{1-p(real\mid w)}\mathbb{P}^f(w)
\end{equation}
1. Compare the estimated values of $$\mathbb{P}^r(w)$$ to the real ones (they're pretty close).

A few notes about the implementation:
- Once you have $$\hat{\theta}$$, you can just as well plug this directly into equation \eqref{eq:softmax_ex} in place of $$\theta^\ast$$ and compute your estimates of $$\mathbb{P}^r(w)$$ that way.  this won't give the same answer (as doing this explicitly normalizes the estimated probabilities where the method used in the notebook does not), but it'll be close.
- The implementation contains a parameter $$\nu$$, which controls the ratio of how many fake observations to generate for each real observation.  If this is set to something other than 1, then the RHS of \eqref{eq:backout} needs to be multiplied by $$\nu$$.  As a result, the implementation in the notebook of \eqref{eq:backout} contains an extra $$\nu$$ term.


### 4. More Details
To be precise, NCE says you can do either of these things 
1. MLE estimation of a distribution parameterized by $$\theta$$ on real data, and
2. Logistic regression on real+fake data parameterized by $$\theta$$ plus a constant term.  

And the resulting estimate of $$\theta$$ that we get will be the same asymptotically.  That is, if we parameterize our model of the true distribution as

$$
P(w\mid \theta) = \frac{Q(w\mid\theta)}{\sum_{v\in W}Q(v\mid\theta)}
$$

and we let $$X_r$$ and $$X_f$$ denote the real and fake data respectively, then we have

$$
\left[\arg\max_{\theta} \left(\sum_{w\in X_r}\log P(w\mid\theta)\right)\right]  
\approx
\left[
    \arg\max_{\theta,c} \left(\sum_{w\in X_r}\log\left(\frac{1}{1+\exp(-\log Q(w\mid\theta)-c)}\right) \\
    \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad
    + \sum_{w\in X_f}\log\left(\frac{1}{1+\exp(\log Q(w\mid\theta)+c)}\right)\right)
\right]
$$

So that we can work with the right side problem because it allows us to just with the un-normalized $$Q$$ instead of the normalized $$P$$.

Several notable things about NCE:
- NCE works pretty much whenever maximum likelihood works.  The conditions for identification / consistency / asymptotic normality are quite weak, and look very similar to those for MLE.
- In order for NCE to work, the fake distribution must include the support of the real distribution.  You can see this from \eqref{eq:backout}: clearly if $$\mathbb{P}^f(w)=0$$, then you're not going to be able to back out $$\mathbb{P}^r(w)$$.
- The further $$\mathbb{P}^f(w)$$ is from $$\mathbb{P}^r(w)$$, the more difficult it gets to accurately estimate $$\mathbb{P}^r(w)$$.  Intuitively, if there's a large gap between these two, then your logistic regression isn't really incentivized to get this difference exactly right.  So, try and have a fake distribution that resembles the real one (e.g. in our example above, it might be reasonable to use the overall occurrence of words on Wikipedia as the fake distribution).
- The more fake observations you generate relative to real observations, the more accurate your estimate becomes.  In the limit, it becomes as accurate as MLE!  Of course, this comes at the cost of increased computational burden, which is why we didn't like MLE in the first place.



### 5. Conditional distributions
- Estimating a single distribution is nice, but...
- Typically, what we care about is actually estimating distributions conditional on some context.  Examples:
  - Predicting the next word in a sentence conditional on the last few words.
  - Predicting in which geographical region the next taxi ride request is going to occur conditional on recent history of rides.
- This is a problem, because NCE introduces a normalizing parameter $$c$$ in additional to the original $$\theta$$ parameters of the likelihood.  
- Typically, contexts are not often repeated in a data set.
- This causes the number of parameters in your model to grow about as quickly as data size, which makes NCE infeasible.  
- In spite of this, NCE finds usage in some fairly high-profile machine learning applications, e.g. word2vec. [See section 2.2 of this paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
  - In that implementation, the normalizing parameter $$c$$ appears to be set just uniformly to 1 for all contexts ([see this paper](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf)), which seems like it just corresponds to fitting a normalized probability model. 
    - These neural network models have a lot of parameters, so this is ok.
    - But why do NCE? Why not just straight up learn your normalized model?
    - I guess the probabilities are not forced to uniformly sum to 1 for all parameter values, so this is some way of coaxing the model to eventually be normalized as we update.
  - Furthermore, the actual implementation of word2vec uses a simplified version of NCE called 'negative sampling' that doesn't have the nice theoretical properties of nce.
    - In the case of learning word embeddings, you're less interested in consistency and more interested in whether your objective function can generate useful embeddings, so you're happy to do something a bit ad-hoc so long as it works.
- At some point in the future I'll probably look into this in more detail.


