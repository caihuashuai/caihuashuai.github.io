---
layout: post
title:  "Double machine learning"
date:   2018-07-22 00:00:00 -0400
categories: statistics, machine learning
---

[Interesting paper by some fairly serious MIT-related econometrics/statistics people](https://arxiv.org/abs/1608.00060) (let's call this paper DML) on how to estimate some low-dimensional parameters of interest in the presence of high-dimensional nuisance parameters.  The intuition is quite nice, though the actual theoretical results end up being not all that useful in practice.

### Setup
- we want to estimate some finite-dimensional parameter $$\theta$$
- there is a (potentially inifinite dimensional) nuisance parameter $$\eta$$ that we don't care about
- the true value of $$\theta$$ and $$\eta$$ are $$\theta_0$$ and $$\eta_0$$
- we have some condition $$\phi$$ that is zero only at the true parameter values:
$$
\begin{equation}
\mathbb{E}[\phi(W, \theta_0, \eta_0)] = 0
\tag{1}
\label{momentcondition}
\end{equation}
$$
where data $$W$$ is generated IID from some distribution $$\mathbb{P}$$ and $$\mathbb{E}$$ denote the expectation wrt $$\mathbb{P}$$
- for a concrete example, think about the case of a simple linear regression:
	- $$\phi$$ is the derivative of the least squares objective
	- $$\theta$$ and $$\eta$$ are the coefficients
	- \eqref{momentcondition} is just the first order conditions, which holds only at the true parmeter values

TODO: add naive and DML algorithms here


### If we didn't need to worry about $$\eta$$
If we knew the true $$\theta_0$$, we could just plug it into the empirical analogue of \eqref{momentcondition} and solve for $$\theta$$ to get an estimate $$\hat{\theta}_n$$:

$$ \mathbb{E}_n[\phi(X, \hat{\theta}_n, \eta_0)] = 0 $$

where $$\mathbb{E}_n$$ denotes the sample average (i.e. expectation wrt the empirical measure).  For sufficiently large $$n$$, $$\hat{\theta}_n$$ will be pretty close to $$\theta_0$$ and $$\phi$$ will be approximately linear, so we can get the asymptotic distribution of $$\hat{\theta}$$:

$$
\begin{align}
0 &= \mathbb{E}_n\phi(X, \hat{\theta}_n, \eta_0) 
\approx \mathbb{E}_n\phi(X, \theta_0, \eta_0) + \partial_{\theta}\mathbb{E}_n[\phi(W, \theta_0, \eta_0)](\hat{\theta}_n-\theta_0) \newline
&\Rightarrow \partial_{\theta}\mathbb{E}_n[\phi(W, \theta_0, \eta_0)] \sqrt{n} (\hat{\theta}_n-\theta_0) \approx - \sqrt{n}\mathbb{E}_n\phi(X, \theta_0, \eta_0) \newline
&\Rightarrow \sqrt{n} (\hat{\theta}_n-\theta_0) \rightarrow_d N(0, J^{-1}\Omega J^{-1\prime})
\tag{2}
\label{expansion0}
\end{align}
$$

where $$J:= \partial_{\theta}\mathbb{E}[\phi(W, \theta_0, \eta_0)]$$ and $$\Omega := \mathrm{Var}\left[\phi(W, \theta_0, \eta_0)\right]$$.

That is, assuming we know the true nuisance parameter $$\eta_0$$, our estimator $$\hat{\theta}_n$$ is $$\sqrt{n}$$-consistent, which is pretty nice.



### But we do need to worry about $$\eta$$
Unfortunately:
- we don't know $$\eta_0$$
- we still need to estimate $$\theta_0$$
- we'd still like to get this same $$\sqrt{n}$$-consistency 

To estimate $$\theta_0$$ in this case, probably the most natural thing to do is:
1. get an estimate $$\hat{\eta}_n$$ of $$\eta$$ using our data
2. pretend $$\hat{\eta}_n \approx \eta_0$$, and then do the same thing we did before where we solve for $$\hat{\theta}_n$$ using the empirical analogue of \eqref{momentcondition}:
$$
\begin{equation}
\mathbb{E}_n[\phi(X, \hat{\theta}_n, \hat{\eta}_n)] = 0 
\tag{3}
\label{empiricalmomentcondition}
\end{equation}
$$ 

Note that so long as this nuisance parameter estimate $$\hat{\eta}_n$$ converges to the truth and $$\phi$$ is smooth, the argmax wrt $$\theta$$ of $$\mathbb{E}_n[\phi]$$ should also be smooth, so we should still get $$\hat{\theta}\rightarrow\theta_0$$.  So we can expand \eqref{empiricalmomentcondition} around $$\theta_0$$:

$$
\begin{align}
0 &= \mathbb{E}_n[\phi(X, \hat{\theta}_n, \hat{\eta}_n)] \approx \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] 
		+  \partial_{\theta}\mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] (\hat{\theta}_n-\theta_0) \newline
&\Rightarrow \partial_{\theta}\mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] (\hat{\theta}_n-\theta_0) \sqrt{n}
		\approx - \sqrt{n} \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)]  
\tag{4}
\label{expansion1}
\end{align}
$$

Basically, this looks a lot like \eqref{expansion0}, except with some $$\hat{\eta}_n$$s instead of $$\eta_0$$.  If we could make this look exactly like \eqref{expansion0} as $$n$$ gets big, then we would get $$\sqrt{n}$$-consistency of $$\hat{\theta}_n$$ even in this case where we don't know the true $$\eta_0$$ and have to plug in $$\hat{\eta}_n$$.
- the left hand side is easy:
	- $$\partial_{\theta}\mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] $$ is just an average 
	- so it should easily go to $$J$$ as $$n$$ gets big so long as $$\phi$$ is smooth and $$\hat{\eta}_n$$ converges to $$\eta$$
	- so the left side of \eqref{expansion1} will look like the left side of \eqref{expansion0} as $$n$$ gets big 
- the right hand side $$\sqrt{n} \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)]$$ is more involved:
	- in order to make this resemble the right side of \eqref{expansion0}, we need 
	$$\sqrt{n} \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] - \sqrt{n} \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_0)] \rightarrow_p 0$$
	- the standard way of doing this is via some kind of stochastic equicontinuity argument (see Andrews 1994 in the references below)
	- but that argument minimally requires the set of potential $$\eta$$ to have finite VC dimension, whereas in modern ML applications typically we'll fit increasingly complex functions as sample size increases, so this kind of classical argument doesn't work

So instead, let's continue with \eqref{expansion1} and expand the RHS:

$$
\begin{align}
& \partial_{\theta}\mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)] (\hat{\theta}_n-\theta_0) \sqrt{n} \newline
&\approx - \sqrt{n} \mathbb{E}_n[\phi(X, \theta_0, \hat{\eta}_n)]  \newline
&\approx - \sqrt{n} 
			\mathbb{E}_n
			\left[
			\phi(X, \theta_0, \eta_0)
			+ \partial_{\eta}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]
			+ \frac{1}{2}\partial_{\eta^2}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]
			\right]
\tag{5}
\label{expansion2}
\end{align}
$$
where :
- $$\partial_{\eta}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]$$ is the directional derivative of $$\phi$$ wrt $$\eta$$ in the direction of $$[\hat{\eta}-\eta_0]$$ 
- $$\partial_{\eta^2}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]$$ the second derivative in that direction
- we've included the second derivative here since it's not ex-ante obvious that it'll vanish


In order to make \eqref{expansion2} look like \eqref{expansion0} (and thus get $$\sqrt{n}$$-consistency for $$\hat{\theta}_n$$), we just need 
$$\sqrt{n} \mathbb{E}_n[\partial_{\eta}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]]$$ and 
$$\sqrt{n} \mathbb{E}_n[\partial_{\eta^2}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]]$$ to both go to 0.  The DML paper gives some conditions under which this is the case.  Roughly:
1. the **Neyman Orthogonality** condition pretty much just amounts to assuming that $$\sqrt{n} \mathbb{E}_n[\partial_{\eta}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]]$$ goes to 0
2. **Cross-Fitting** + assumption that $$\lvert\lvert\hat{\eta}-\eta_0\rvert\rvert_2 = o_p(n^{-1/4})$$ gives us $$\sqrt{n} \mathbb{E}_n[\partial_{\eta^2}\phi(X, \theta_0, \eta_0)[\hat{\eta}-\eta_0]]$$ going to 0
	- the $$\lvert\lvert\hat{\eta}_n-\eta_0\rvert\rvert_2 = o_p(n^{-1/4})$$ guarantees that for randomly chosen IID samples, $$\mathbb{E}_n[\partial_{\eta^2}\phi(X, \theta_0, \eta_0)[\hat{\eta}_n-\eta_0]] = o_p(n^{-1/2})$$
		- to see this, just consider the case where $$\eta$$ is 1-dimensional, where this second directional derivative is just $$ \partial_{\eta^2}\phi(X, \theta_0, \eta_0) (\hat{\eta}_n-\eta_0)^2$$
		- basically the same thing in general so long as $$\phi$$ is smooth
	- the cross-fitting just means that we estimate $$\hat{\eta}_n$$ on a sample that we then don't re-use when estimating $$\theta$$, so that we essentially have IID samples

Some more verbal intuition is warranted here:
- that we need a Neyman orthogonality condition seems reasonable: 
	- we're plugging in an estimate of the nuisance parameter $$\eta$$ to stand in for the truth $$\eta_0$$ when we estimate $$\theta$$
	- so if the equation we solve to estimate $$\theta$$ depends on the value of this nuisance parameter then small errors in the nuisance parameter will mess things up
	- Neyman orthogonality just says that this depedence gets small as our data gets big
- estimating $$\eta$$ on an auxiliary data set separate from the data we use to estimate $$\theta$$ is basically to just control for overfitting
	- if we used the same data for estimating the $$\eta$$ as $$\theta$$, then the expectations in \eqref{expansion2} would all be relative to the data that $$\eta$$ was estimated on
	- as a result, these in-sample estimates of $$\eta$$ could overfit, and thus might not converge to $$\eta_0$$ at the required rate
	- so if we don't do cross-fitting, but rather find other ways to prevent overfitting, then everything should still be fine
		- the DML authors have some other work where they take this approach

### Practical notes


### References
- [Double/Debiased Machine Learning for Treatment and Causal Parameters](https://arxiv.org/abs/1608.00060)
- [Andrews 1994 Asymptotics for Semiparametric Econometric Models via Stochastic Equicontinuity](http://dido.econ.yale.edu/~dwka/pub/p0863.pdf)
- [Luo Spindler linear boosting convergence rate](https://arxiv.org/abs/1602.08927)
- [Wager Walther tree concentration rate](https://arxiv.org/abs/1503.06388)
- [Chen White neural network convergence rate](https://ieeexplore.ieee.org/document/749011/)